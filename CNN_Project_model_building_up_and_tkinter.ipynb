{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\William\\Miniconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 148, 148, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 72, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 72, 72, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 34, 34, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18496)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                1183808   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,212,513\n",
      "Trainable params: 1,212,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Found 1800 images belonging to 2 classes.\n",
      "Found 480 images belonging to 2 classes.\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "#step1 import libaray\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D,  ZeroPadding2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import sys\n",
    "# Plot inline\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# step2 dimension of our image\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "#train_data = \"D:/project_data/train\"\n",
    "#validation_data = \"D:/project_data/validation\"\n",
    "train_data = \"D:/project_data/treat/train\"\n",
    "validation_data = \"D:/project_data/treat/validation\"\n",
    "train_samples = 1800\n",
    "#train_samples = 120\n",
    "validation_samples = 480\n",
    "#validation_samples = 36\n",
    "epochs = 10\n",
    "batch_size = 24\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3,img_width, img_height )\n",
    "else:\n",
    "    input_shape = (img_width, img_height,3)\n",
    "    \n",
    "model = Sequential()\n",
    "#model.add(ZeroPadding2D(padding=(3, 3))(input_shape)\n",
    "#model.add(ZeroPadding2D(padding=(3, 3), input_shape = input_shape))\n",
    "model.add(Conv2D(32,(3,3), input_shape = input_shape))\n",
    "#model.add(ZeroPadding2D(padding=(3, 3), data_format=None))\n",
    "#model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, \n",
    "                             #gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
    "#model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001,input_shape = input_shape))\n",
    "#model.add(BatchNormalization(axis=-1,input_shape = input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "#model.add(ZeroPadding2D(padding=(3, 3))(input_shape = input_shape)\n",
    "#model.add(ZeroPadding2D(padding=(3, 3), input_shape = input_shape))\n",
    "model.add(Conv2D(32,(3,3)))\n",
    "#model.add(ZeroPadding2D(padding=(3, 3), data_format=None))\n",
    "#model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, \n",
    "                             #gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
    "#model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "#model.add(ZeroPadding2D(padding=(3, 3))(input_shape = input_shape)          \n",
    "#model.add(ZeroPadding2D(padding=(3, 3), input_shape = input_shape))\n",
    "#model.add(Conv2D(64,(3,3), strides=(1, 1)))\n",
    "#model.add(ZeroPadding2D(padding=(3, 3), data_format=None))\n",
    "#model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, \n",
    "                             #gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
    "#model.add(BatchNormalization(axis=-1))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "#model.add(ZeroPadding2D(padding=(3, 3))(input_shape = input_shape)\n",
    "#model.add(ZeroPadding2D(padding=(3, 3), input_shape = input_shape))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "#model.add(ZeroPadding2D(padding=(3, 3), data_format=None))\n",
    "#model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, \n",
    "                             #gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
    "#model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "#model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "#Save the model after every epoch.\n",
    "callbacks = []\n",
    "callbacks.append(ModelCheckpoint('weight.{epoch:05d}-{val_acc:.5f}.h5', monitor='val_acc', save_best_only=True, period=1))\n",
    "\n",
    "\n",
    "                #train_generator,train_history=model.fit_generator(\n",
    "train_history=model.fit_generator(\n",
    "              train_generator,\n",
    "              steps_per_epoch=train_samples//batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=validation_generator,\n",
    "              validation_steps=validation_samples//batch_size)\n",
    "\n",
    "#train_history = model.fit_generator(\n",
    "                #steps_per_epoch=train_samples,\n",
    "                #epochs=epochs,\n",
    "                #validation_data=validation_generator,\n",
    "                #validation_steps=validation_samples,\n",
    "                 #verbose=0, workers=2,\n",
    "                 #callbacks=callbacks) #verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
    "                                       #workers: maximum number of processes to spin up\n",
    "\n",
    "#說明:evaluate_generator(self, generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "#score = model.evaluate_generator(validation_generator, validation_samples/batch_size, workers=12)\n",
    "score = model.evaluate_generator(validation_generator, validation_samples/batch_size)\n",
    "\n",
    "#說明:predict_generator(self, generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
    "#scores = model.predict_generator(validation_generator, validation_samples/batch_size, workers=12)\n",
    "scores = model.predict_generator(validation_generator, validation_samples/batch_size)\n",
    "\n",
    "pd.crosstab(validation_generator,scores, rownames=['label'], colnames=['predict'])\n",
    "\n",
    "correct = 0\n",
    "for i, n in enumerate(validation_generator.filenames):\n",
    "    if n.startswith(\"NG\") and scores[i][0] <= 0.5:\n",
    "        correct += 1\n",
    "    if n.startswith(\"OK\") and scores[i][0] > 0.5:\n",
    "        correct += 1\n",
    "\n",
    "print(\"Correct:\", correct, \" Total: \", len(validation_generator.filenames))\n",
    "#print(\"Loss: \", score[0], \"Accuracy: \", score[1])\n",
    "print(\"Loss: \", score[0], \"\\t[Info] Accuracy of Validation data = {:2.1f}%\".format(score[1]*100.0))\n",
    "\n",
    "def show_train_history(train_history, train, validation):\n",
    "    plt.plot(train_history.history[train])\n",
    "    plt.plot(train_history.history[validation])\n",
    "    plt.title('Train history')\n",
    "    plt.ylabel(train)\n",
    "    plt.xlabel('Epoch')\n",
    "    legendLoc = 'lower right' if(train=='acc') else 'upper right'\n",
    "    plt.legend(['train', 'validation'], loc=legendLoc)\n",
    "    plt.show()\n",
    "show_train_history(train_history, 'acc', 'val_acc')\n",
    "show_train_history(train_history, 'loss', 'val_loss')\n",
    "\n",
    "model.save_weights('first_test_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_47 (Conv2D)           (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 148, 148, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_47 (MaxPooling (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_48 (Conv2D)           (None, 72, 72, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 72, 72, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_48 (MaxPooling (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_49 (Conv2D)           (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 34, 34, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_49 (MaxPooling (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 18496)             0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 64)                1183808   \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,212,513\n",
      "Trainable params: 1,202,369\n",
      "Non-trainable params: 10,144\n",
      "_________________________________________________________________\n",
      "Found 1800 images belonging to 2 classes.\n",
      "Found 480 images belonging to 2 classes.\n",
      "Epoch 1/20\n",
      "75/75 [==============================] - 31s 416ms/step - loss: 0.3093 - acc: 0.8850 - val_loss: 1.1534 - val_acc: 0.7000\n",
      "Epoch 2/20\n",
      "75/75 [==============================] - 27s 359ms/step - loss: 0.2520 - acc: 0.9044 - val_loss: 1.4185 - val_acc: 0.6958\n",
      "Epoch 3/20\n",
      "75/75 [==============================] - 26s 353ms/step - loss: 0.2785 - acc: 0.8961 - val_loss: 1.3720 - val_acc: 0.6708\n",
      "Epoch 4/20\n",
      "75/75 [==============================] - 27s 355ms/step - loss: 0.2567 - acc: 0.9011 - val_loss: 1.5723 - val_acc: 0.6437\n",
      "Epoch 5/20\n",
      "75/75 [==============================] - 25s 331ms/step - loss: 0.2532 - acc: 0.9094 - val_loss: 1.5335 - val_acc: 0.6396\n",
      "Epoch 6/20\n",
      "75/75 [==============================] - 25s 335ms/step - loss: 0.2552 - acc: 0.9083 - val_loss: 1.6418 - val_acc: 0.6708\n",
      "Epoch 7/20\n",
      "75/75 [==============================] - 24s 327ms/step - loss: 0.2271 - acc: 0.9261 - val_loss: 1.4698 - val_acc: 0.6562\n",
      "Epoch 8/20\n",
      "75/75 [==============================] - 27s 361ms/step - loss: 0.2285 - acc: 0.9272 - val_loss: 1.4860 - val_acc: 0.6646\n",
      "Epoch 9/20\n",
      "75/75 [==============================] - 25s 337ms/step - loss: 0.2227 - acc: 0.9233 - val_loss: 1.5900 - val_acc: 0.6604\n",
      "Epoch 10/20\n",
      "75/75 [==============================] - 26s 350ms/step - loss: 0.2095 - acc: 0.9300 - val_loss: 1.6629 - val_acc: 0.6667\n",
      "Epoch 11/20\n",
      "75/75 [==============================] - 25s 339ms/step - loss: 0.2203 - acc: 0.9272 - val_loss: 1.6089 - val_acc: 0.6729\n",
      "Epoch 12/20\n",
      "75/75 [==============================] - 25s 329ms/step - loss: 0.2083 - acc: 0.9244 - val_loss: 1.4386 - val_acc: 0.6667\n",
      "Epoch 13/20\n",
      "75/75 [==============================] - 25s 328ms/step - loss: 0.2112 - acc: 0.9261 - val_loss: 1.5940 - val_acc: 0.6792\n",
      "Epoch 14/20\n",
      "75/75 [==============================] - 25s 333ms/step - loss: 0.2114 - acc: 0.9294 - val_loss: 1.5597 - val_acc: 0.6813\n",
      "Epoch 15/20\n",
      "75/75 [==============================] - 25s 329ms/step - loss: 0.1985 - acc: 0.9261 - val_loss: 1.8443 - val_acc: 0.6729\n",
      "Epoch 16/20\n",
      "75/75 [==============================] - 25s 327ms/step - loss: 0.1920 - acc: 0.9317 - val_loss: 1.9191 - val_acc: 0.6833\n",
      "Epoch 17/20\n",
      "75/75 [==============================] - 24s 323ms/step - loss: 0.1727 - acc: 0.9411 - val_loss: 2.0226 - val_acc: 0.6667\n",
      "Epoch 18/20\n",
      "75/75 [==============================] - 25s 329ms/step - loss: 0.1962 - acc: 0.9328 - val_loss: 1.7000 - val_acc: 0.6708\n",
      "Epoch 19/20\n",
      "75/75 [==============================] - 26s 342ms/step - loss: 0.1756 - acc: 0.9422 - val_loss: 1.5218 - val_acc: 0.6833\n",
      "Epoch 20/20\n",
      "75/75 [==============================] - 25s 330ms/step - loss: 0.1945 - acc: 0.9328 - val_loss: 1.2344 - val_acc: 0.7021\n",
      "Loss:  1.234373116493225 \t[Info] Accuracy of Validation data = 70.2%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xl4VOXZ+PHvnX0lCUmAkAWCICI7\nBLR1w1or4oJbEXet1taqrX1tf9q3m9r9fbv4Wq0WW1prXUAURaq1LiiiogSFyCo7CQkQAgkJ2ZP7\n98dzQoaYZEKYyQS4P9c1V+ac85w590ySc8+znOeIqmKMMcZ0JizUARhjjOn9LFkYY4zxy5KFMcYY\nvyxZGGOM8cuShTHGGL8sWRhjjPHLkoUxnRCRcBGpEpGcbuw7VEQ6HJsuIj8WkceOLEJjeobYdRbm\nWCIiVT6LcUAd0OQtf0NVn+rBWIYCG1RVjvB1lgB/UdW/ByQwY7ohItQBGBNIqprQ8lxEtgK3qOob\nHZUXkQhVbeyJ2EJBRMIAVLU51LGYo5s1Q5njioj8XETmiMgzIlIJXCsiXxCRpSJSLiIlIvKQiER6\n5SNEREVksLf8T2/7qyJSKSIfiEiun2NeLyJFIlIqIve2ieXv3vM4EXlaRMq8OD4SkTQR+Q3wBeAx\nrznsQa/86SKSLyIVXtlTfF53iYj8TEQ+AA4A94jIh21iukdE5h35J2qOF5YszPHoUuBpIAmYAzQC\n3wHSgNOAqcA3Otn/auDHQF9gO/AzP8f7IjAUOA+4X0SGtVPmJlyzWRaQCnwLqFXVe4APgG+qaoKq\n3iUiacC/gN95ZR8CXhGRFJ/Xuw74GtAHeBgY3ua41wJP+onbmIMsWZjj0RJVfVlVm1W1RlWXqeqH\nqtqoqpuBWcBZnew/T1XzVbUBeAoY5+d496lqrap+DKwGxrZTpgGXrIaqapP3+lXtlAO4CFitqs94\nMf8T2Axc4FNmtqquVdUGVa0EnsMlCERkHJABvOInbmMOsmRhjkeFvgsicpKI/EtEdorIfuAB3Im7\nIzt9nlcDCR0VBFDVrpT/O/AGMFdEdojIr0Wkoz7FgcC2Nuu2AZk+y4Vttj8BXOM9vxaY4yU7Y7rE\nkoU5HrUdAvhnYBXuW30f4CfAEY1gOuyAVOtV9T5VHQGcjmsqazm5t423GBjUZl0OsMP3Jdu8/hIA\nETkNuAprgjKHyZKFMZAIVAAHRGQEnfdXBIWIfElERnmjl/bjmqVahvzuAob4FF8IjBSRK70O+Ktx\nfSL+mpWeBB4FDqjq0sC+A3Oss2RhDNwN3ABU4moZc0IQw0DgBVyiWI1rknrG2/YgcJU3Sur3qloK\nXAzcA5QB3wUuVNW9fo7xD2AUVqsw3WAX5RlznBCReGA3MEpVt4Q6HnN0sZqFMceP24H3LFGY7rAr\nuI05DohIEa4fZHqoYzFHJ2uGMsYY45c1QxljjPHrmGmGSktL08GDB4c6DGOMOaosX758j6qm+yt3\nzCSLwYMHk5+fH+owjDHmqCIibWcDaFdQm6FEZKqIrBeRjb6zbfpsHyQib4pIgYi8LSJZPtuaRGSF\n91gQzDiNMcZ0Lmg1CxEJBx4BzgWKgGUiskBV1/gU+y3wD1V9QkS+BPwKN1smQI2q+pugzRhjTA8I\nZs1iMrBRVTeraj3wLJ8ftncy8Kb3fFE7240xxvQCwUwWmRw682URh86KCbASuNx7fimQKCKp3nKM\nd3OXpSJySXsHEJFbvTL5paWlgYzdGGOMj2Ami/Zm7Wx7Ucf3gLNE5BPc/QN24G5EA5Cjqnm4G808\nKCInfO7FVGepap6q5qWn++3MN8YY003BHA1VBGT7LGfhplY+SFWLgcsARCQBuFxVK3y2oaqbReRt\nYDywKYjxGmOM6UAwaxbLgGEikisiUcBM4JBRTd49hlti+AEw21ufIiLRLWVwt7r07Rg3xhjTg4KW\nLFS1EbgDeA1YC8xV1dUi8oCIXOwVmwKsF5HPgP7AL7z1I4B8EVmJ6/j+dZtRVMYYc9SrqW/ipRU7\nmP9JEdX1jf53CKFjZm6ovLw8tYvyjDFd1dysvLdpD88vLwJg2ugMzhqeTnREeFCPq6qsLKpgzrJC\nXl5ZTFWdSxLxUeFcNHYgMyZlMz47GZGeuVmjiCz3+oc7dcxcwW2MMV1RuLeaecuLmLe8iB3lNSTF\nRhIm8OKKYhKjIzh3ZH8uGjOQ04amERURuMaXsqo65n+yg7n5hXy2q4qYyDCmjc7gyrxswsKEOcsK\neWlFMc8uK2RYvwRm5GVz6YRM0hKiAxbDkbCahTHmmFfb0MRrq3fyXH4R723aA8DpQ9OYkZfNuSf3\nJzxMeH9TGQtXFvPa6p3sr20kKTaSqSMHcOHYDL4wJJWI8MNPHE3NyuLPSpmbX8gba3fR0KSMy05m\nRl42F43NIDEm8pDyVXWNLFxZzNz8Qj7eXk5EmHDOiH5cOSmbM4eldysGf7pas7BkYYw5Zq3aUcFz\n+YW8uKKYipoGMpNj+WpeFldMzCIrJa7dfeobm3l3QykLC0p4fc0uquoaSY2PYuqoAVw4ZiCTc/sS\nHtZ5E9G2sgM8l+9qLzv319I3PopLx2cyIy+b4QMSuxT7hl2VzM0v5IWPd1B2oJ7+faK5fEIWX83L\nJjct/rA/i45YsjDGHJcqqht4aeUO5iwrZHXxfqIiwpg6cgAz8rL54gmphPk50fuqbWji7fWlLCwo\n5s21u6lpaCI9MZoLRmdw4ZgMJuSkHHy9mvomXl1Vwtz8QpZu3kuYwFknpjMjL5tzRvTvdpNWfWMz\nb63bzdz8Qt5ev5tmhcm5fZmRl8200QOIizqy3gRLFsaY40Zzs/L+pjLm5hfy79U7qW9s5uSMPlw5\nKZvp4waSHBd1xMeorm/krXW7WbiyhEXrd1PX2ExGUgwXjM6guqGJl1cUU1nXSE7fOGbkZXH5xCwy\nkmID8O5a7dpfy7zlRTyXX8jWsmoSoiO4aGwGM/KyGZ+T0q3XtGRhjDmmlVXV8fH2cvK37eVfBSUU\n7auhT0wEl3jNPaMyk4J27Kq6Rt5Ys4uFBcW881kpYSJMG+1O2qfk9j2s2kt3qCofbdnLnPxCXvm0\nhKH9Elh45xndei1LFsYEWXOzsnbnft7fWEazKhnJsQxMiiEjOZb+idFB6YzsbZqblV2VtUSEhZGW\nEBW04Z7NzcqG3VUs37aP5dv28fH2fWzZcwCAiDDhlCGuWea8kQOIiQzu0Ne2KmsbCBMhPjo0g0sr\naxvYWVHLsP5d6wtpy4bOGhMEu/bX8u6GPby7oZQlG/ZQdqC+3XJhAv0SY8hIjmFgUiwZXhIZ6PMz\nLSE66N9AA6G2oYnCvdVs31vNtrKWnwfYvreawn011Dc2AxAXFU5O3zhy+sYxKDWOnNR497xvHJkp\nsUQeRvKsrG1gZWGFSw7b9/HJ9n1U1rrrEVLjo5gwKIUrJ2UzcVAKozOTejxB+Go7oikUx++JGCxZ\nmF6hoamZ6vomqusb3c+61ucHDq5r5EB9EzXeOveziZr6RlLjozmhXzxD+yUwND2RzJRYvyNWuqK6\nvpEPt+zl3c/2sGRjKZ/tqgIgLSGKM09M5/ShaZw+LI24qHBKKmopLq+hpKKWkvIaiitqKamoYW3J\nft5ct4vahuZDXjsyXOjfx0smyTFkJMUy0PuZkRTDwORYUuIig35xlqqyr7qhNQmUVbPNSw7by6rZ\nub/2kPLxUeHkpMYzrF8iXx7Rn+y+cTQ2Nbt9yqrZvOcAb39WejCJAISHCQOTY7xkEs+gVJdEsr3E\nsu9AA8u37/VqDuWs37mfZgURGN4/kYvGDmRiTgoTB6UwKDWuxy5YM62sGcqE1NLNZfy/eQVs31vd\n5X3CBOKjIoiLDic+KoLoyHBKK2vZU9X6LT86IozcNJc8TkhPOPhzSHp8p99Cm5uV1cX7eXdjKe9+\ntofl2/ZR39RMdEQYk3P7csawNE4fms5JAxIPq1agqpRXN1BcUUNJuUsixW2Sys6KWhqaDv1/jIkM\nO5g8Dkkmya1Jpo/3rbKxqZn9tY2UV9dTUdNwyKO8+tDn+1ue17iybRNZv8RoVzvwTuw5fePI8U7w\nfeP9Nzc1Nyu7K+vYVnbgYBLZvtdLQmUH2Ffd0O5+CdERjM9JZoKXGMblJB98fyY4rM/C9GqNTc08\n9OYGHl60kZy+cVw2IYu4qHDioyOIiwonLiqC+Khw4g4uu3VxUeFER4S1e7Iqr65nU2kVG3dXsan0\ngPezisK91TR7f+YikJUSy1CfBJKbFs+2smre3biH9zbuYa/XtDQiow9nDEvjjGFpTBrcN+hNHc3N\nyp4Dda3JpE1SKamoZdf+2oPvpUVCdAQCVNZ1PrdQXFQ4ybGR9ImNJDkukqTYSJJjo0iKi/SSg0sM\n2SlxxEYF973ur21oTSBl1STERJA3KIUT+ycGpEZous6Shem1CvdWc9ecFSzfto/LJ2Rx//SRJASx\nc7C2oYmtZS55+CaSzaVV1Pk0lfRLjOb0YWmcOSyd04amkZ7YO6ZZ8NXY1MzuyrpDk0m5ayY6mAC8\nn+4RdfB5IKeuMMcO6+A2vdLCgmJ+8MKnqML/zRzH9HFtb54YeDGR4Zw0oA8nDehzyPrmZmVHeQ2b\nSqsYkBTD8P6Jvb4tPCI8jIHJsQxMjmXioFBHY44nlixMj6iub+T+BWuYk1/IuOxkHpo5npzU9qdb\n6ClhYUK218lqjOmcJQsTdKuLK7jzmU/YsucA35pyAt8998TDGkZpjAk9SxYmaFSV2e9t5TevriMl\nPpKnbj6FLw5NC3VYxphusGRhgmJPVR3ff24li9aX8uUR/fifK8bSN/7I5+cxxoSGJQsTcO9uKOW/\n5q6koqaB+y8eyfVfGNTrO46NMZ2zZGECpr6xmd+9vp4/v7OZYf0S+MfXJjMio4//HY0xvZ4lC0NV\nXSNNTUpsVHi3x+JvKzvAt5/5hJVFFVx9Sg4/vuDkoF/YZYzpOZYsjlMV1Q28tnonLxcU8/6mMpq8\ny4Ijw+XgldK+V03HR0cQGxXurqr2XRcZTk1DE39atJGI8DAeu3YCU0dlhPjdGWMCzZLFcaSytoHX\n1+xiYUEJ724opaFJye4byy1n5NIvMYbqukaqGz4/YV91fRO7K2u9yf1a1zX5zDsxeXBfHpw5joHJ\ngb3ZizGmd7BkcYyrrm/kjbW7Wbiy+OBMoAOTYrjxi4O5cMxAxmQldavzWVWpb2qmpr6J2oZm+veJ\ntk5sY45hliyO0MbdVfzylbX07xPNhWMGckpu35Df9Ka2oYlF63azsKDk4NTY/RKjuXpyDheNzWB8\ndsoR30dBRIiOCCc6wvoljDkeBDVZiMhU4P+AcOAvqvrrNtsHAbOBdGAvcK2qFnnbbgB+5BX9uao+\nEcxYu+O11Tu5e+5KwsOEhqZmnvmokLSEKM4f5W7mPmlw8G+v2KKusYnFn+1hYUExb6zZxYH6JlLj\no7hiYhYXjhnIpMF9bTZPY0y3BW3WWREJBz4DzgWKgGXAVaq6xqfMc8BCVX1CRL4E3KSq14lIXyAf\nyAMUWA5MVNV9HR2vJ2edbWpW/vD6Zzy8aCNjs5J49NqJpMRFsWj9bhYWFPPWut0Hm2amjc7gwjED\nmZCTHNBmmtqGJtaU7OfTogo+2b6PN9ftprK2keS4SKaOHMCFYwZy6pDQ13KMMb1bb5h1djKwUVU3\newE9C0wH1viUORn4rvd8EfCi9/w84HVV3evt+zowFXgmiPF2SUV1A99+9hPe+ayUK/OyuX/6yIP3\nOZg2OoNpozM4UNfIG2tdR/JTS7fzt/e2kpkcywVjXI1jdObh9RPUNzazfmclBTvK+bSogoKiCj7b\nVUmj18GclhDFV04ewIVjMzh9aJrNu2SMCbhgJotMoNBnuQg4pU2ZlcDluKaqS4FEEUntYN/PzWUt\nIrcCtwLk5OQELPCOrC3ZzzeeXE5JRQ2/uHQUV0/OafekHx8dwfRxmUwfl8n+2gZeX72LhQXFzF6y\nhVmLNzMoNY4LvBrHiIxDp8VubGpmw+4qlxR2lFNQVMG6kkrqm9x9F5LjIhmdmcQ3ThrC6MxkxmYn\nMaBPjHUuG2OCKpjJor2zV9s2r+8BD4vIjcBiYAfQ2MV9UdVZwCxwzVBHEqw/C1YWc8+8AvrERvDs\nrV9g4qCULu3XJyaSyydmcfnELMqr6/nP6l28XFDMnxdv5k9vb2JIejznjxrAgbomCorKWVOy/+At\nLhOjIxiVmcRNpw1mTFYyY7KSyEqJtcRgjOlxwUwWRUC2z3IWUOxbQFWLgcsARCQBuFxVK0SkCJjS\nZt+3gxhrhxqbmvn1q+v4y5ItTBqcwiPXTKBfYky3Xis5LooZk7KZMSmbsqo6/r16JwtXlvCntzcR\nExHOqMw+XHPKIMZkJTE6M4nBqfE91kFujDGdCWYHdwSug/scXI1hGXC1qq72KZMG7FXVZhH5BdCk\nqj/xOriXAxO8oh/jOrj3dnS8YHRwl1XVccfTn/DB5jJu+MIgfnjByUG5NWVlbQNxURE2WskY0+NC\n3sGtqo0icgfwGm7o7GxVXS0iDwD5qroAV3v4lYgorhnqdm/fvSLyM1yCAXigs0QRDCsLy7ntn8sp\nO1DP7746lssnZgXtWIkxkUF7bWOMCYSg1Sx6WiBrFnOXFfKjl1aRnhDNn6+byKjMpIC8rjHG9DYh\nr1kcjeobm7n/5dU89eF2Th+axkNXjbcb9hhjDJYsDtq1v5bb/rmcj7eX842zhvD9rwy3C9qMMcZj\nyQJYtnUv33rqYw7UNfLI1RO4YIxNsW2MMb6O+2SxcXcVV81aSlZKLP+8+RSGD0gMdUjGGNPrHPfJ\nYmi/BO6fPpILxwwkKdZGJRljTHuO+2QBcM0pg0IdgjHG9GrWg2uMMcYvSxbGGGP8smRhjDHGL0sW\nxhhj/LJkYYwxxi9LFsYYY/yyZGGMMcYvSxbGGGP8smRhjDHGL0sWxhhj/LJkYYwxxi9LFsYYY/yy\nZGGMMcYvSxbGGGP8smRhjDHGL0sWxhhj/LJkYYwxxq+gJgsRmSoi60Vko4jc2872HBFZJCKfiEiB\niEzz1g8WkRoRWeE9HgtmnMYYYzoXtNuqikg48AhwLlAELBORBaq6xqfYj4C5qvqoiJwMvAIM9rZt\nUtVxwYrPGGNM1wWzZjEZ2Kiqm1W1HngWmN6mjAJ9vOdJQHEQ4zHGGNNNwUwWmUChz3KRt87XfcC1\nIlKEq1Xc6bMt12ueekdEzmjvACJyq4jki0h+aWlpAEM3xhjjK5jJQtpZp22WrwL+rqpZwDTgSREJ\nA0qAHFUdD/wX8LSI9GmzL6o6S1XzVDUvPT09wOEbY4xpEcxkUQRk+yxn8flmppuBuQCq+gEQA6Sp\nap2qlnnrlwObgBODGKsxxphOBDNZLAOGiUiuiEQBM4EFbcpsB84BEJERuGRRKiLpXgc5IjIEGAZs\nDmKsxhhjOhG00VCq2igidwCvAeHAbFVdLSIPAPmqugC4G3hcRL6La6K6UVVVRM4EHhCRRqAJ+Kaq\n7g1WrMYYYzonqm27EY5OeXl5mp+fH+owjDHmqCIiy1U1z185u4LbGGOMX5YsjDHG+GXJwhhjjF+W\nLIwxxvhlycIYY4xfliyMMcb4ZcnCGGOMX5YsjDHG+GXJwhhjjF+WLIwxxvhlycIYY4xfliyMMcb4\nZcnCGGOMX5YsjDHG+GXJwhhjjF+WLIwxxvhlycIYY4xfliyMMcb4ZcnCGGOMX5YsjDHG+GXJwhhj\njF+WLIwxxvhlycIYY4xfliyMMcb4FdRkISJTRWS9iGwUkXvb2Z4jIotE5BMRKRCRaT7bfuDtt15E\nzgtmnMYYYzoXEawXFpFw4BHgXKAIWCYiC1R1jU+xHwFzVfVRETkZeAUY7D2fCYwEBgJviMiJqtoU\nrHiNMcZ0LJg1i8nARlXdrKr1wLPA9DZlFOjjPU8Cir3n04FnVbVOVbcAG73XM8YYEwJdShYicqmI\nJPksJ4vIJX52ywQKfZaLvHW+7gOuFZEiXK3izsPYFxG5VUTyRSS/tLS0K2/FGGNMN3S1ZvFTVa1o\nWVDVcuCnfvaRdtZpm+WrgL+rahYwDXhSRMK6uC+qOktV81Q1Lz093U84xhhjuqurfRbtJRV/+xYB\n2T7LWbQ2M7W4GZgKoKofiEgMkNbFfY0xxvSQrtYs8kXk9yJygogMEZE/AMv97LMMGCYiuSISheuw\nXtCmzHbgHAARGQHEAKVeuZkiEi0iucAw4KMuxmqMMSbAupos7gTqgTnAXKAGuL2zHVS1EbgDeA1Y\nixv1tFpEHhCRi71idwNfF5GVwDPAjeqs9o6zBvg3cLuNhDLGmNAR1c91BRyV8vLyND8/P9RhGGPM\nUUVElqtqnr9yXR0N9bqIJPssp4jIa0cSoDHGmKNHV5uh0rwRUACo6j6gX3BCMsYY09t0NVk0i0hO\ny4KIDKadoazGGGOOTV0dOvtDYImIvOMtnwncGpyQjDHG9DZdShaq+m8RycMliBXAS7gRUcYYY44D\nXUoWInIL8B3cxXErgFOBD4AvBS80Y4wxvUVX+yy+A0wCtqnq2cB43MVzxhhjjgNdTRa1qloLICLR\nqroOGB68sIwxxvQmXe3gLvKus3gReF1E9mFzNRljzHGjqx3cl3pP7xORRbh7T/w7aFEZY4zpVQ77\nTnmq+o7/UsYYY44lQb0HtzHGmGODJQtjjDF+WbIwxhjjlyULY4wxflmyMMYY45clC2OMMX5ZsjDG\nGOOXJQtjjDF+WbIwxhjjlyULY4wxflmyMMYY45clC2OMMX5ZsjDGGONXUJOFiEwVkfUislFE7m1n\n+x9EZIX3+ExEyn22NflsWxDMOI0xxnTusKco7yoRCQceAc4FioBlIrJAVde0lFHV7/qUvxN3u9YW\nNao6LljxGWOM6bpg1iwmAxtVdbOq1gPPAtM7KX8V8EwQ4zHGGNNNwUwWmUChz3KRt+5zRGQQkAu8\n5bM6RkTyRWSpiFzSwX63emXyS0tLAxW3McaYNoKZLKSdddpB2ZnAPFVt8lmXo6p5wNXAgyJywude\nTHWWquapal56evqRR2yMMaZdwUwWRUC2z3IWUNxB2Zm0aYJS1WLv52bgbQ7tzzDGGNODgpkslgHD\nRCRXRKJwCeFzo5pEZDiQAnzgsy5FRKK952nAacCatvsaY4zpGUEbDaWqjSJyB/AaEA7MVtXVIvIA\nkK+qLYnjKuBZVfVtohoB/FlEmnEJ7de+o6iMMcb0LDn0HH30ysvL0/z8/FCHYYwxRxURWe71D3fK\nruA2xhjjlyULY4wxflmyMMYY45clC2OMMX5ZsjDGGOOXJQtjjDF+WbIwxhjjlyULY4wxflmyMMYY\n45clC2OMMX5ZsjDGGOOXJQtjjDF+WbIwxhjjlyULY4wxflmyMMYY45clC2OMMX5ZsjDGGOOXJQtj\njDF+WbIwxhjjlyULY4wxflmyMMYY45clC2OMMX5ZsjDGGONXUJOFiEwVkfUislFE7m1n+x9EZIX3\n+ExEyn223SAiG7zHDcGM0xhjTOcigvXCIhIOPAKcCxQBy0RkgaquaSmjqt/1KX8nMN573hf4KZAH\nKLDc23dfsOI1xpijjip88DDUVcLZ/x3UQwWzZjEZ2Kiqm1W1HngWmN5J+auAZ7zn5wGvq+peL0G8\nDkwNSpSqsPpF2L0WmhqCcghjjAm4mnKYcy3850fu/NXcHNTDBa1mAWQChT7LRcAp7RUUkUFALvBW\nJ/tmtrPfrcCtADk5Od2LsnInPOe1coVFQupQ6Dei9ZE+AvrmQlh4917fGGMCraQA5l4PFYVw3i/h\n1G+BSFAPGcxk0V7k2kHZmcA8VW06nH1VdRYwCyAvL6+j1+5cfBp8c4nLzC2P4o9h9QutZSJiIG0Y\n9DsZ0k9yP/udBEk5EGZjBIwxPUQVPnkS/vU9iEuFG1+BnHa/gwdcMJNFEZDts5wFFHdQdiZwe5t9\np7TZ9+0AxtYqPBIGjHYPX/UHoHS9Sx6lXhLZ+h4UzGktExkP6cOh/8lw2l0uoRhjTDDUV8O/7oaV\nT8OQs+Hyv7gvuz1EVLv3hdzvC4tEAJ8B5wA7gGXA1aq6uk254cBrQK56wXgd3MuBCV6xj4GJqrq3\no+Pl5eVpfn5+wN/H59RWwO51rQlk91rY8TFExsJNr1jCMMYE3p6Nrtlp9xo46x446/8FrGlcRJar\nap6/ckGrWahqo4jcgUsE4cBsVV0tIg8A+aq6wCt6FfCs+mQtVd0rIj/DJRiABzpLFD0qJslV+3yr\nfqXr4e8XwBMXwY3/gtQTQhefMebYsno+vHSnawW59nkYek5IwghazaKn9VjNoiO71sATF7r+jRv/\n5TrFjTGmuxrr4fUfw4ePQdZk+OrfICkr4Ifpas3CemcDpf/JcP1L0FDtahjl20MdkTHmaFVeCH87\n3yWKU7/lvoAGIVEcDksWgTRgNFz3ItTth79fCBVFoY7IGHO02fAG/PkM17z91Sdg6q8gIirUUVmy\nCLiB4+C6+VCzzyWM/R0NADPGGB/NTfDWL+CpK6BPJtz6Noy8JNRRHWTJIhgyJ8K1L8CBPa5JqnJn\nqCMyxvRmVaXw5KWw+H9g3DVw8+uQNjTUUR3CkkWwZE+Ca+fB/hJ44mKo2h3qiIw59jQ3w+Z33ACT\no9Xmt12zU+GHcPHDcMkjEBUX6qg+x5JFMOWcCtfMdZ3d/5gOB8pCHZExgdfU4K4s7km1FfDBn+Dh\nifCPi+Gx0+DNn7kRREeLwo/cF8l/THfXad3yBky4LtRRdciGzvaEze/A0zMgdRjcsADi+oY6ImM6\n1tzk+twOlPo8ytos74HqPe55bYVrYz/5Ehh1mWuGDcA8RQ0NDRQVFVFbW9u6sqkB6ivd1czaDOHR\nEJ0AjXVQXwXhUe7/Kzz0HcIdaqx3n1ljDUg4xPSBqISgz+0UExNDVlYWkZGRh6zv6tBZSxY9ZeOb\n8MxVbk6p61+C2JRQR2SMG4Dx/sOws8AlgAOlULPXnYjbkjA3H1FcmptmIj7dPeL6QslK2PgGNNVD\ncg6MvBRGXgYZY7t9EtyyZQuJiYmk9u2L1O13sdVXAeL+f+LTD22uqa1wtfjmJkgcAAn9g34CPiz1\n1VBZ4kZLSriLLz6tRyYpVVXysuh2AAAWkElEQVTKysqorKwkN/fQa8AsWfRGG16HZ6+G/qPg+hfd\n1eDGhELlLljyB8ifDdoEmXmQkO4lAi8JHEwI3s/YlM5PbDXlsP4VWPUCbF4EzY3Qd4hLGqMucxNw\nHsbJe+2a1ZyUlYrUlLkkFBbpYolLdVczt6ep0c3EWlsOkXGQPAgiYw7zwwmwhhqXJGorvCTRz32e\nPTyTtaqybt06RowYcch6Sxa91fpXYc51bojttS+4KqgxPeXAHnjvQfjoL+4EPO4qOPP7kDI4sMep\n3gtrX3azN29Z7GoqacNd0hh5GaSf2PG+JQXw0Z9Z2386I3LSXRNNfLr7ctXVZFOzz13Yps3QZ6Db\nv6drGQ21XpIod0kiPt0l5LBgzt/aubVr11qyOGqSBbh/ork3QNYkN9dLdEKoIzLHuuq98P4f4cM/\nu7by0TPcZHQ9MY9ZVSmsfQlWzYdt7wHqatcjL3XJo+8Q1xex9mX4aBZs/wAi41g7dR4jxkxwnb/d\n0dTgmqXq9ruEk5wDEdEBfWvtaqx1w+Vr9rmmu/h0iO8H4aFLEi0sWXCUJQtwk4PNuxlyvuBGTEXF\nhzoi09PqKmH7UncCje4DuWdCxrjAnlRqymHpn9zIofoqd3I+6x43tX4o7C+BNS+5Gkfhh25dxlg3\ntLyyxNVwJn0dxl/D2q07P3diO2yqrg+mYgegriM+LrVLtYzy8nKefvppvvWtb3XtWI11ULmTaVdc\nw9MP/4rkzCEQ379XJIkWliw4CpMFwKfz4IWvw+DT4ao5vXJsdZeUb3f/gJbwOldf7U6QWxbD1nfd\n1Pba5JolmhtdmahEGPRFyD0DBp/hppDpTtt27X5Xi/jgj66tfMTFMOUHbg6z3qK8ENa8CGsWuCam\nSbfAsHMPvt/2Tmzd1ljv/k7rKyE60dUy/IyY2rp1KxdeeCGrVq06ZH1TUxPh4d7vRJtdTeLAHld7\nA9evktC/436VEDqSZNF7Ut7xaPQV7iQx/5sw7ya48qle9S2kS4ry3YRncWlw3s9de3RvGoESSg21\nULTMJYYt77rnzQ0uOQycAKff5WoTWZPdzba2vttadsNr7jVikt2XicFnuASSPqLzuzPWH3BNOe/9\nn2sGGT4Nptzrvr33NsnZ8MU73cOP+19ezZri/Ud+zOYGaNwDbOXkrBR+esmEDv9e7733XjZt2sS4\nceOIjIwkIT6OjP7prFhZwJr3X+OSa26hcEcJtXV1fOfmq7n11lshsT+DTziR/Px8qqqqOP/88zn9\n9NN5//33yczM5KWXXiI2tpvNaiF2lJ2ZjkFjZ7p/8H/9F7z8HZj+8NFzsq0qdZ31iQPcN8N5X4P8\nv8G0/3X3Lz/eNDW42sKWxbB1sbvoqrHWtVtnjIVTb4Pcs9zFmm37qaLiXBPRqMvc8v4SL3F4tZB1\nC936uFSXPHLPhMFnupttibhaS/5sN8Kpeg8MPRfO/oG75sG0CouEyHDXZFRXCfu2QFJ2ay1A1XX8\nN9Ty6x/dzaqVn7DiP8/y9uIlXHD9nax66zlyczKhoZrZf/xf+vYbQE2DMumsr3D5zXeR2qa2smHD\nBp555hkef/xxZsyYwfPPP8+1114bgjd+5CxZ9AaTbnZttu/82g2r+/JPQx2Rf02NrjZUsxdu/o/r\nsMyfDW/9DB49DU75pvtGeyyP9mpqdNcXbF3sagPbl0LDAbet/2jI+5o7qed8AWKTD++1+2TAmBnu\nAa4JZYtPzWPNS259wgCXfLZ/AFW7YMgUOPuHkD05UO+yV/jpRSMD+4KqcGC3S8r161zTVGOtSyIt\n15gc2O1d+BcJMUlMnjSR3Alfcp3kYeE89Oh9zJ8/H4DCwiI2bNhAamrqIYfJzc1l3LhxAEycOJGt\nW7cG9n30IEsWvcWUe6FqJyz5vWvvPPWboY6oc2/e505clzzW2sQx+etuhMubD7hO1U+fg6/8DMZc\nGdzaUlWpm8I52NetNDfDrk9bT9rb3ncjbQDST4JxV3vf+E8P/FX6yTkw/hr3UIW9m1sTx/YPIO1E\nuOJvMPi0wB73WCXi/s+i+7i+k7oqd+OyuAT3MzIWavu4xJB6AiQUEp+YfLBf8e233+aNN97ggw8+\nIC4ujilTphx6pbknOrp19FV4eDg1NTU99hYDzZJFbyECF/zedZT9+17XSTb6ilBH1b7V890wzEm3\nuHH6vuLT4OKHYOIN8Mr3Yf43WpumMsYELobKne7b9aoXoHCpW9cn0520+41ofaSf1P2Od1V3j/WD\nzUFL3Jh5gL4nuCaj3DNdf0JCv8C8r64QcSew1BNg4o09d9xjUWRsh9d8JCYlU1lZ2e62iooKUlJS\niIuLY926dSxdujSYUfYKlix6k7BwuPyv8M/LXKd3XCqccHaoozrU7nXw4u2uU/a8X3VcLnMi3PwG\nrPgnvHEfzDoL8m6Gs/+7+9+6D+zxhl3OdyduFPqNdM0u4ZHuxL57LXy0BJrqWvdLHuSTPLyfaSd+\n/speVSjb2NpPsHWJm2IC3Df7ky5srTkkZXbvPZijRmpqKqeddhqjRo0iNjaW/v37H9w2depUHnvs\nMcaMGcPw4cM59dRTQxhpz7Chs71RTTn8bRqUb4MbF8LA8aGOyKndD4+f7YZifmOxuzK2K2r2waJf\nwrK/uCkjzvkpjL+u81E9Lar3us7dVS1XAje5CRlHXe6avPqd9Pl9mptg31bYvcYlt91roHQd7Nng\nRsOA63TuO8TVPNKHu6aIre+6sf4AiQNdYmgZwpoyqGvv1QRMQIfOGsCuswCOsWQBruPtr19x9/S+\n+T89c6VtZ1RhzrVuupIbFrhv14dr56euaWr7B27o6LTfQlY7o3VqK2Ddvw6dYyglt3WqiP4ju9cH\n0tQAZZtak0dLTWTvJleLa2lSyj3TJZKjZVTaMcqSReDZdRbHoj4ZcN0LLmE8eam7c1Zif//7BcuS\nP7hv+Of9snuJAtwFZje9CgVz4fUfw1/OcfP3n/NT15G4/t/uyt6W2UuTst3N6kdd5q5sPtKTd3ik\nq4m0rY00edc+WHIwpkOWLHqztGFwzTx44kJ46nK48ZXQDEXdtMgNiR15mTt5HwkRGHslDD8f3vkN\nfPgYrH7RJYfGWkjMcB3nIy+DrLyeOYH3wittjeltLFn0dlkTYcaT8MyVbnrza5/vmcnQWpQXwvM3\nuxlDL/5j4E7eMX3gvF+4vovF/+uuQxh5mbsmoSt9GcaYHhXU/0oRmSoi60Vko4jc20GZGSKyRkRW\ni8jTPuubRGSF91gQzDh7vWFfhul/ch2wL3zddeD2hIZamHuda6a58p/BmR2330lwxV/hgt+5awQs\nURjTKwWtZiEi4cAjwLlAEbBMRBao6hqfMsOAHwCnqeo+EfEdrF6jquOCFd9RZ+yV7orS//wIXr3H\nXbcQ7CaaV78PxZ/AzKchbWhwj2WM6dWC+TVuMrBRVTeraj3wLDC9TZmvA4+o6j4AVd0dxHiOfi2T\nri17HN79bXCPtfwJ+PgfcMbdcNIFwT2WMceAhARX8y4uLuaKK9q/oHbKlCn4G7X54IMPUl1dfXB5\n2rRplJeXBy7QbgpmssgECn2Wi7x1vk4EThSR90RkqYhM9dkWIyL53vpL2juAiNzqlckvLS0NbPS9\n1ZcfgDEz4a2fuxN6MOxYDq98D074krvgzRjTZQMHDmTevHnd3r9tsnjllVdITj7MucWCIJgd3O21\nkbS9qCMCGAZMAbKAd0VklKqWAzmqWiwiQ4C3RORTVd10yIupzgJmgbvOItBvoFcKC3Mz01aXwcK7\n3PQagfzmf2APzLneTVB3+V97/D7BxrTr1XvddTqBNGA0nP/rDjffc889DBo06ODNj+677z5EhMWL\nF7Nv3z4aGhr4+c9/zvTphzaY+N4Ho6amhptuuok1a9YwYsSIQ+aGuu2221i2bBk1NTVcccUV3H//\n/Tz00EMUFxdz9tlnk5aWxqJFixg8eDD5+fmkpaXx+9//ntmzZwNwyy23cNddd7F169YemQo9mDWL\nIiDbZzkLKG6nzEuq2qCqW4D1uOSBqhZ7PzcDbwO95DLmXiA8EmY84a7snvc1N6FdIDQ1utc7UApX\nPhn4yfCMOYrMnDmTOXPmHFyeO3cuN910E/Pnz+fjjz9m0aJF3H333XR2YfOjjz5KXFwcBQUF/PCH\nP2T58uUHt/3iF78gPz+fgoIC3nnnHQoKCvj2t7/NwIEDWbRoEYsWLTrktZYvX87f/vY3PvzwQ5Yu\nXcrjjz/OJ598Arip0G+//XZWr15NcnIyzz//fIA/jeDWLJYBw0QkF9gBzASublPmReAq4O8ikoZr\nltosIilAtarWeetPA/4niLEefaLi4ernYPZ58MxMd6vMfie7eY8S+nev8/utn8GWd2D6IzDQxhaY\nXqSTGkCwjB8/nt27d1NcXExpaSkpKSlkZGTw3e9+l8WLFxMWFsaOHTvYtWsXAwYMaPc1Fi9ezLe/\n/W0AxowZw5gxrZNpzp07l1mzZtHY2EhJSQlr1qw5ZHtbS5Ys4dJLLyU+3k2Medlll/Huu+9y8cUX\n98hU6EFLFqraKCJ3AK8B4cBsVV0tIg8A+aq6wNv2FRFZAzQB31fVMhH5IvBnEWnG1X5+7TuKynji\nU91V3k9eBq/9d+v62JTWCfN8J9CLT+34tdYsgPcehIk3wfij8+YsxgTaFVdcwbx589i5cyczZ87k\nqaeeorS0lOXLlxMZGcngwYPbnZrcl7TzxW3Lli389re/ZdmyZaSkpHDjjTf6fZ3OajA9MRV6UC/K\nU9VXgFfarPuJz3MF/st7+JZ5HxgdzNiOGck5cGe+u6dD6drW+Y52r3X3+K6raC0b38+b7uJkbyrv\nk91y5S548TY3U+z5vwndezGml5k5cyZf//rX2bNnD++88w5z586lX79+REZGsmjRIrZt29bp/mee\neSZPPfUUZ599NqtWraKgoACA/fv3Ex8fT1JSErt27eLVV19lypQpACQmJlJZWUlaWtrnXuvGG2/k\n3nvvRVWZP38+Tz75ZFDed3vsCu5jRUK6e+Se2bpO1c2ienD21bUuoXz8ZOsd3QDCo92dwmb8o2ev\nDjemlxs5ciSVlZVkZmaSkZHBNddcw0UXXUReXh7jxo3jpJPamfXYx2233cZNN93EmDFjGDduHJMn\nuzsYjh07lvHjxzNy5EiGDBnCaae13rTq1ltv5fzzzycjI+OQfosJEyZw4403HnyNW265hfHjx/fY\n3fds1tnjUXMzVBS2Jo99W2HC9Xa/ZtOr2KyzgWezzprDExbm7s+QMgiGT/Vf3hhz3LOJeIwxxvhl\nycIY02sdK83kvcGRfpaWLIwxvVJMTAxlZWWWMAJAVSkrKyMmJsZ/4Q5Yn4UxplfKysqiqKiI42be\ntyCLiYkhKyur2/tbsjDG9EqRkZHk5uaGOgzjsWYoY4wxflmyMMYY45clC2OMMX4dM1dwi0gp0PlE\nLZ1LA/YEKJxgsPiOjMV3ZCy+I9Ob4xukqun+Ch0zyeJIiUh+Vy55DxWL78hYfEfG4jsyvT2+rrBm\nKGOMMX5ZsjDGGOOXJYtWs0IdgB8W35Gx+I6MxXdkent8flmfhTHGGL+sZmGMMcYvSxbGGGP8Oq6S\nhYhMFZH1IrJRRO5tZ3u0iMzxtn8oIoN7MLZsEVkkImtFZLWIfKedMlNEpEJEVniPn7T3WkGOc6uI\nfOod/3O3JhTnIe8zLBCRCT0Y23Cfz2aFiOwXkbvalOnRz1BEZovIbhFZ5bOur4i8LiIbvJ8pHex7\ng1dmg4jc0IPx/a+IrPN+f/NFJLmDfTv9WwhifPeJyA6f3+G0Dvbt9P89iPHN8Yltq4is6GDfoH9+\nAaWqx8UDCAc2AUOAKGAlcHKbMt8CHvOezwTm9GB8GcAE73ki8Fk78U0BFob4c9wKpHWyfRrwKiDA\nqcCHIfx978RdcBSyzxA4E5gArPJZ9z/Avd7ze4HftLNfX2Cz9zPFe57SQ/F9BYjwnv+mvfi68rcQ\nxPjuA77Xhd9/p//vwYqvzfbfAT8J1ecXyMfxVLOYDGxU1c2qWg88C0xvU2Y68IT3fB5wjohITwSn\nqiWq+rH3vBJYC2T2xLEDbDrwD3WWAskikhGCOM4BNqnqkVzVf8RUdTGwt81q37+zJ4BL2tn1POB1\nVd2rqvuA14GA3wO3vfhU9T+q2ugtLgW6P6/1Eerg8+uKrvy/H7HO4vPOHTOAZwJ93FA4npJFJlDo\ns1zE50/GB8t4/ywVQGqPROfDa/4aD3zYzuYviMhKEXlVREb2aGCOAv8RkeUicms727vyOfeEmXT8\nTxrqz7C/qpaA+5IA9GunTG/5HL+Gqym2x9/fQjDd4TWTze6gGa83fH5nALtUdUMH20P5+R224ylZ\ntFdDaDtuuCtlgkpEEoDngbtUdX+bzR/jmlXGAn8EXuzJ2DynqeoE4HzgdhE5s8323vAZRgEXA8+1\ns7k3fIZd0Rs+xx8CjcBTHRTx97cQLI8CJwDjgBJcU09bIf/8gKvovFYRqs+vW46nZFEEZPssZwHF\nHZURkQggie5VgbtFRCJxieIpVX2h7XZV3a+qVd7zV4BIEUnrqfi84xZ7P3cD83HVfV9d+ZyD7Xzg\nY1Xd1XZDb/gMgV0tTXPez93tlAnp5+h1qF8IXKNeA3tbXfhbCApV3aWqTaraDDzewXFD/flFAJcB\nczoqE6rPr7uOp2SxDBgmIrneN8+ZwII2ZRYALaNOrgDe6ugfJdC89s2/AmtV9fcdlBnQ0ociIpNx\nv7+ynojPO2a8iCS2PMd1hK5qU2wBcL03KupUoKKlyaUHdfiNLtSfocf37+wG4KV2yrwGfEVEUrxm\nlq9464JORKYC9wAXq2p1B2W68rcQrPh8+8Au7eC4Xfl/D6YvA+tUtai9jaH8/Lot1D3sPfnAjdT5\nDDdK4ofeugdw/xQAMbimi43AR8CQHoztdFw1uQBY4T2mAd8EvumVuQNYjRvZsRT4Yg9/fkO8Y6/0\n4mj5DH1jFOAR7zP+FMjr4RjjcCf/JJ91IfsMcUmrBGjAfdu9GdcP9iawwfvZ1yubB/zFZ9+veX+L\nG4GbejC+jbj2/pa/w5YRggOBVzr7W+ih+J70/rYKcAkgo2183vLn/t97Ij5v/d9b/uZ8yvb45xfI\nh033YYwxxq/jqRnKGGNMN1myMMYY45clC2OMMX5ZsjDGGOOXJQtjjDF+WbIw5jCISFObmW0DNpup\niAz2nb3UmN4kItQBGHOUqVHVcaEOwpieZjULYwLAuzfBb0TkI+8x1Fs/SETe9Ca9e1NEcrz1/b17\nRaz0Hl/0XipcRB4Xd0+T/4hIbMjelDE+LFkYc3hi2zRDXemzbb+qTgYeBh701j2Mm7J9DG5Cvoe8\n9Q8B76ib0HAC7ipegGHAI6o6EigHLg/y+zGmS+wKbmMOg4hUqWpCO+u3Al9S1c3ehJA7VTVVRPbg\npqNo8NaXqGqaiJQCWapa5/Mag3H3sBjmLd8DRKrqz4P/zozpnNUsjAkc7eB5R2XaU+fzvAnrVzS9\nhCULYwLnSp+fH3jP38fNeApwDbDEe/4mcBuAiISLSJ+eCtKY7rBvLcYcnlgRWeGz/G9VbRk+Gy0i\nH+K+hF3lrfs2MFtEvg+UAjd5678DzBKRm3E1iNtws5ca0ytZn4UxAeD1WeSp6p5Qx2JMMFgzlDHG\nGL+sZmGMMcYvq1kYY4zxy5KFMcYYvyxZGGOM8cuShTHGGL8sWRhjjPHr/wMI+ylUwDY3WwAAAABJ\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23b43a54400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvvx/cYqB2NVlVmgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23b45c8de48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#step1 import libaray\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, ZeroPadding2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, advanced_activations,Input\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.applications.imagenet_utils import preprocess_input \n",
    "from keras import backend as K\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "#random.seed(3)\n",
    "%matplotlib inline\n",
    "# step2 dimension of our image\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "#train_data = \"D:/project_data/train\"\n",
    "#validation_data = \"D:/project_data/validation\"\n",
    "train_data = \"D:/project_data/treat/train\"\n",
    "validation_data = \"D:/project_data/treat/validation\"\n",
    "model_weights_path = 'first_test_1.h5'\n",
    "train_samples = 1800\n",
    "#train_samples = 120\n",
    "validation_samples = 480\n",
    "#validation_samples = 36\n",
    "epochs = 20\n",
    "batch_size = 24\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3,img_width, img_height )\n",
    "else:\n",
    "    input_shape = (img_width, img_height,3)\n",
    "\n",
    "    \n",
    "model = Sequential()\n",
    "#model.add(ZeroPadding2D(padding=(3, 3), input_shape = input_shape))\n",
    "model.add(Conv2D(32,(3,3), input_shape = input_shape))\n",
    "#model.add(Conv2D(32,(3,3), input_shape = input_shape))\n",
    "#model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, \n",
    "                             #gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
    "#model.add(BatchNormalization(axis=-1))\n",
    "#model.add(advanced_activations.LeakyReLU(alpha=0.3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "#model.add(ZeroPadding2D(padding=(3, 3), input_shape = input_shape))\n",
    "model.add(Conv2D(32,(3,3)))\n",
    "#model.add(Conv2D(32,(3,3)))\n",
    "#model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, \n",
    "                            #gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
    "#model.add(BatchNormalization(axis=-1))\n",
    "#model.add(advanced_activations.LeakyReLU(alpha=0.3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "#model.add(ZeroPadding2D(padding=(3, 3), input_shape = input_shape))\n",
    "#model.add(Conv2D(64,(3,3), strides=(1, 1), input_shape = input_shape))\n",
    "#model.add(Conv2D(64,(3,3)))\n",
    "#model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, \n",
    "                             #gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
    "#model.add(BatchNormalization(axis=-1))\n",
    "#model.add(advanced_activations.LeakyReLU(alpha=0.3))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "#model.add(ZeroPadding2D(padding=(3, 3), input_shape = input_shape))\n",
    "#model.add(Conv2D(64,(3,3), strides=(1, 1), input_shape = input_shape))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "#model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, \n",
    "                             #gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
    "#model.add(BatchNormalization(axis=-1))\n",
    "#model.add(advanced_activations.LeakyReLU(alpha=0.3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "#model.add(Flatten(input_shape=[1,150,150]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(advanced_activations.LeakyReLU(alpha=0.3))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "#model.add(advanced_activations.Softmax(axis=1))\n",
    "\n",
    "model.load_weights(model_weights_path)\n",
    "\n",
    "\n",
    "for layer in model.layers[:6]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "train_history=model.fit_generator(\n",
    "              train_generator,\n",
    "              steps_per_epoch=train_samples//batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=validation_generator,\n",
    "              validation_steps=validation_samples//batch_size)\n",
    "\n",
    "score = model.evaluate_generator(validation_generator, validation_samples/batch_size)\n",
    "scores = model.predict_generator(validation_generator, validation_samples/batch_size)\n",
    "\n",
    "print(\"Loss: \", score[0], \"\\t[Info] Accuracy of Validation data = {:2.1f}%\".format(score[1]*100.0))\n",
    "\n",
    "def show_train_history(train_history, train, validation):\n",
    "    plt.plot(train_history.history[train])\n",
    "    plt.plot(train_history.history[validation])\n",
    "    plt.title('Train history')\n",
    "    plt.ylabel(train)\n",
    "    plt.xlabel('Epoch')\n",
    "    legendLoc = 'lower right' if(train=='acc') else 'upper right'\n",
    "    plt.legend(['train', 'validation'], loc=legendLoc)\n",
    "    plt.show()\n",
    "show_train_history(train_history, 'acc', 'val_acc')\n",
    "show_train_history(train_history, 'loss', 'val_loss')\n",
    "\n",
    "#model.save_weights('first_test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\William\\Miniconda3\\lib\\site-packages\\keras\\preprocessing\\image.py:492: UserWarning: This ImageDataGenerator specifies `zca_whitening`, which overrides setting of `featurewise_center`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "C:\\Users\\William\\Miniconda3\\lib\\site-packages\\keras\\preprocessing\\image.py:571: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "C:\\Users\\William\\Miniconda3\\lib\\site-packages\\keras\\preprocessing\\image.py:589: UserWarning: This ImageDataGenerator specifies `zca_whitening`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    }
   ],
   "source": [
    "#for image or data augmentation\n",
    "\n",
    "#import image generator\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "#image treatment factor\n",
    "#datagen = ImageDataGenerator(rotation_range=40,\n",
    "                            #width_shift_range=0.2,\n",
    "                            #height_shift_range=0.2,\n",
    "                            #shear_range=0.2,\n",
    "                            #zoom_range=0.2,\n",
    "                            #horizontal_flip=True,\n",
    "                            #fill_mode='nearest')\n",
    "datagen = ImageDataGenerator(rotation_range=40,\n",
    "                           width_shift_range=0.2,\n",
    "                           height_shift_range=0.2,\n",
    "                            zca_epsilon=1e-06,\n",
    "                             zca_whitening=True,\n",
    "        #rescale=1./255,\n",
    "                             shear_range=0.2,\n",
    "                             zoom_range=0.2,\n",
    "                             horizontal_flip=True,\n",
    "                             fill_mode='nearest') # randomly shift images vertically (fraction of total heigh)                     \n",
    "\n",
    "img = load_img(\"D:/project_data/train/OK/OK_60.jpg\") # this is a PIL immage that we wnat to treat\n",
    "x = img_to_array(img) #this is a numpy array with shape(3,150,150)\n",
    "x = x.reshape((1,)+x.shape) # this is a numpy array with shape(1,3,150,150)\n",
    "\n",
    "# use .flow() to below batch to randomly to transfer image and save to D:/project_data/treat\n",
    "# tansfer 21 pictures for in  case\n",
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=1, \n",
    "                         save_to_dir = 'D:/project_data/treat/For_test_image_modify', \n",
    "                          save_prefix='OK', save_format='jpeg'):\n",
    "    i +=1\n",
    "    if i>20:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#step1 import libaray\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "# step2 dimension of our image\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data = \"D:/project_data/treat/train\"\n",
    "validation_data = \"D:/project_data/treat/validation\"\n",
    "train_samples = 1800\n",
    "validation_samples = 480\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3,img_width, img_height )\n",
    "else:\n",
    "    input_shape = (img_width, img_height,3)\n",
    "    \n",
    "model = Sequential()\n",
    "#model.add(Conv2D(32,(3,3), input_shape = input_shape))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#model.add(Conv2D(32,(3,3)))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#model.add(Conv2D(32,(3,3)))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#model.add(Conv2D(64, (3, 3)))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "for layer in model.layers[:6]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "train_history=model.fit_generator(\n",
    "              train_generator,\n",
    "              steps_per_epoch=train_samples,\n",
    "              epochs=epochs,\n",
    "              validation_data=validation_generator,\n",
    "              validation_steps=validation_samples)\n",
    "\n",
    "score = model.evaluate_generator(validation_generator, validation_samples/batch_size)\n",
    "scores = model.predict_generator(validation_generator, validation_samples/batch_size)\n",
    "\n",
    "print(\"Loss: \", score[0], \"\\t[Info] Accuracy of Validation data = {:2.1f}%\".format(score[1]*100.0))\n",
    "\n",
    "def show_train_history(train_history, train, validation):\n",
    "    plt.plot(train_history.history[train])\n",
    "    plt.plot(train_history.history[validation])\n",
    "    plt.title('Train history')\n",
    "    plt.ylabel(train)\n",
    "    plt.xlabel('Epoch')\n",
    "    legendLoc = 'lower right' if(train=='acc') else 'upper right'\n",
    "    plt.legend(['train', 'validation'], loc=legendLoc)\n",
    "    plt.show()\n",
    "show_train_history(train_history, 'acc', 'val_acc')\n",
    "show_train_history(train_history, 'loss', 'val_loss')\n",
    "\n",
    "#model.save_weights('first_test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Found 1800 images belonging to 2 classes.\n",
      "Found 480 images belonging to 2 classes.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_5 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 2,097,665\n",
      "Trainable params: 2,097,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1800 samples, validate on 480 samples\n",
      "Epoch 1/20\n",
      "1800/1800 [==============================] - 3s 2ms/step - loss: 0.9704 - acc: 0.6461 - val_loss: 0.6968 - val_acc: 0.5667\n",
      "Epoch 2/20\n",
      "1800/1800 [==============================] - 2s 1ms/step - loss: 0.5467 - acc: 0.7222 - val_loss: 0.5866 - val_acc: 0.6896\n",
      "Epoch 3/20\n",
      "1800/1800 [==============================] - 2s 1ms/step - loss: 0.4436 - acc: 0.7972 - val_loss: 0.6113 - val_acc: 0.7104\n",
      "Epoch 4/20\n",
      "1800/1800 [==============================] - 2s 1ms/step - loss: 0.3777 - acc: 0.8283 - val_loss: 0.6455 - val_acc: 0.6979\n",
      "Epoch 5/20\n",
      "1800/1800 [==============================] - 2s 1ms/step - loss: 0.3326 - acc: 0.8517 - val_loss: 0.9882 - val_acc: 0.6292\n",
      "Epoch 6/20\n",
      "1800/1800 [==============================] - 2s 1ms/step - loss: 0.2993 - acc: 0.8656 - val_loss: 0.7223 - val_acc: 0.7021\n",
      "Epoch 7/20\n",
      "1800/1800 [==============================] - 2s 1ms/step - loss: 0.2491 - acc: 0.9039 - val_loss: 0.8666 - val_acc: 0.6958\n",
      "Epoch 8/20\n",
      "1800/1800 [==============================] - 2s 1ms/step - loss: 0.2216 - acc: 0.9067 - val_loss: 0.9791 - val_acc: 0.6625\n",
      "Epoch 9/20\n",
      "1800/1800 [==============================] - 2s 1ms/step - loss: 0.2259 - acc: 0.9056 - val_loss: 1.0156 - val_acc: 0.6375\n",
      "Epoch 10/20\n",
      "1800/1800 [==============================] - 2s 1ms/step - loss: 0.2016 - acc: 0.9161 - val_loss: 1.3132 - val_acc: 0.6313\n",
      "Epoch 11/20\n",
      "1800/1800 [==============================] - 2s 1ms/step - loss: 0.1691 - acc: 0.9267 - val_loss: 1.0975 - val_acc: 0.6542\n",
      "Epoch 12/20\n",
      "1800/1800 [==============================] - 2s 1ms/step - loss: 0.1708 - acc: 0.9333 - val_loss: 1.3091 - val_acc: 0.6396\n",
      "Epoch 13/20\n",
      "1800/1800 [==============================] - 2s 1ms/step - loss: 0.1611 - acc: 0.9378 - val_loss: 1.2347 - val_acc: 0.6875\n",
      "Epoch 14/20\n",
      "1800/1800 [==============================] - 2s 1ms/step - loss: 0.1330 - acc: 0.9489 - val_loss: 1.3331 - val_acc: 0.6396\n",
      "Epoch 15/20\n",
      "1800/1800 [==============================] - 2s 1ms/step - loss: 0.1229 - acc: 0.9589 - val_loss: 1.3717 - val_acc: 0.6688\n",
      "Epoch 16/20\n",
      "1800/1800 [==============================] - 2s 1ms/step - loss: 0.1354 - acc: 0.9461 - val_loss: 1.4069 - val_acc: 0.6979\n",
      "Epoch 17/20\n",
      "1800/1800 [==============================] - 2s 1ms/step - loss: 0.1141 - acc: 0.9572 - val_loss: 1.3674 - val_acc: 0.6812\n",
      "Epoch 18/20\n",
      "1800/1800 [==============================] - 2s 1ms/step - loss: 0.1021 - acc: 0.9567 - val_loss: 1.9384 - val_acc: 0.6188\n",
      "Epoch 19/20\n",
      "1800/1800 [==============================] - 2s 1ms/step - loss: 0.1191 - acc: 0.9550 - val_loss: 1.6431 - val_acc: 0.6729\n",
      "Epoch 20/20\n",
      "1800/1800 [==============================] - 2s 1ms/step - loss: 0.0893 - acc: 0.9656 - val_loss: 1.7993 - val_acc: 0.6521\n",
      "480/480 [==============================] - 0s 311us/step\n",
      "Loss:  1.799271007378896 \t[Info] Accuracy of Validation data = 65.2%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "#top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "top_model_weights_path = 'bottleneck_fc_model_1.h5'\n",
    "#top_model_weights_path = 'bottleneck_fc_model_3.h5'\n",
    "#train_data_dir = 'data/train'\n",
    "#validation_data_dir = 'data/validation'\n",
    "train_data_dir = \"D:/project_data/treat/train\"\n",
    "validation_data_dir = \"D:/project_data/treat/validation\"\n",
    "\n",
    "#train_data_dir = \"D:/project_data/train\"\n",
    "#validation_data_dir = \"D:/project_data/validation\"\n",
    "\n",
    "#nb_train_samples = 2000\n",
    "nb_train_samples = 1800\n",
    "#nb_train_samples = 120\n",
    "#nb_validation_samples = 800\n",
    "nb_validation_samples = 480\n",
    "#nb_validation_samples = 40\n",
    "epochs =20\n",
    "batch_size = 24\n",
    "\n",
    "\n",
    "def save_bottlebeck_features():\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    # build the VGG16 network\n",
    "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "    # build the ResNet50 network\n",
    "    #model = applications.ResNet50(include_top=False, weights='imagenet')\n",
    "    print('Model loaded.')\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "        #shuffle=True)\n",
    "    bottleneck_features_train_1 = model.predict_generator(\n",
    "        generator, nb_train_samples // batch_size)\n",
    "    #np.save(open('bottleneck_features_train.npy', 'w'),\n",
    "            #bottleneck_features_train)\n",
    "    np.save('bottleneck_features_train_1.npy', bottleneck_features_train_1)\n",
    "    generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    bottleneck_features_validation_1 = model.predict_generator(\n",
    "        generator, nb_validation_samples // batch_size)\n",
    "    #np.save(open('bottleneck_features_validation.npy', 'w'),\n",
    "            #bottleneck_features_validation)\n",
    "    np.save('bottleneck_features_validation_1.npy', bottleneck_features_validation_1)\n",
    "\n",
    "def train_top_model():\n",
    "    #train_data = np.load(open('bottleneck_features_train.npy'))\n",
    "    train_data = np.load('bottleneck_features_train_1.npy')\n",
    "    train_labels = np.array(\n",
    "        [0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))\n",
    "\n",
    "    #validation_data = np.load(open('bottleneck_features_validation.npy'))\n",
    "    validation_data = np.load('bottleneck_features_validation_1.npy')\n",
    "    validation_labels = np.array(\n",
    "        [0] * (nb_validation_samples // 2) + [1] * (nb_validation_samples // 2))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.7))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    train_history= model.fit(train_data, train_labels,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            validation_data=(validation_data, validation_labels))\n",
    "    score = model.evaluate(validation_data,validation_labels)\n",
    "    #score = model.evaluate(validation_data,validation_labels, nb_validation_samples/batch_size)\n",
    "    #scores = model.predict(validation_data,validation_labels, nb_validation_samples/batch_size)\n",
    "\n",
    "    print(\"Loss: \", score[0], \"\\t[Info] Accuracy of Validation data = {:2.1f}%\".format(score[1]*100.0))\n",
    "\n",
    "    model.save_weights(top_model_weights_path)\n",
    "    \n",
    "\n",
    "save_bottlebeck_features()\n",
    "train_top_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\William\\Miniconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Found 120 images belonging to 2 classes.\n",
      "Found 40 images belonging to 2 classes.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 1)                 2097665   \n",
      "=================================================================\n",
      "Total params: 16,812,353\n",
      "Trainable params: 9,177,089\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "from keras import applications\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# path to the model weights files.\n",
    "#weights_path = '../keras/examples/vgg16_weights.h5'\n",
    "#top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "top_model_weights_path = 'bottleneck_fc_model_1.h5'\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "#train_data_dir = 'data/train'\n",
    "#validation_data_dir = 'data/validation'\n",
    "#train_data_dir = \"D:/project_data/treat/train\"\n",
    "#validation_data_dir = \"D:/project_data/treat/validation\"\n",
    "#test_data_dir = 'test1'\n",
    "\n",
    "train_data_dir = \"D:/project_data/train\"\n",
    "validation_data_dir = \"D:/project_data/validation\"\n",
    "#nb_train_samples = 2000\n",
    "#nb_validation_samples = 800\n",
    "#nb_train_samples = 1800\n",
    "#nb_validation_samples = 480\n",
    "\n",
    "nb_train_samples = 120\n",
    "nb_validation_samples = 36\n",
    "epochs = 20\n",
    "batch_size = 4\n",
    "\n",
    "# build the VGG16 network\n",
    "base_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(150,150,3))\n",
    "#base_model = applications.ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "print('Model loaded.')\n",
    "\n",
    "# build a classifier model to put on top of the convolutional model\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.7))\n",
    "top_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# note that it is necessary to start with a fully-trained\n",
    "# classifier, including the top classifier,\n",
    "# in order to successfully do fine-tuning\n",
    "top_model.load_weights(top_model_weights_path)\n",
    "\n",
    "# add the model on top of the convolutional base\n",
    "# model.add(top_model)\n",
    "model = Model(inputs=base_model.input, outputs=top_model(base_model.output))\n",
    "\n",
    "# set the first 25 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "for layer in model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# fine-tune the model\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size,\n",
    "    verbose=2)\n",
    "#model.save_weights(top_model_weights_path)\n",
    "\n",
    "#計算成績及顯示混淆矩陣\n",
    "score = model.evaluate_generator(validation_generator, nb_validation_samples/batch_size)\n",
    "scores = model.predict_generator(validation_generator, nb_validation_samples/batch_size)\n",
    "\n",
    "\n",
    "#print(\"Loss: \", score[0], \"Accuracy: \", score[1])\n",
    "print(\"Loss: \", score[0], \"\\t[Info] Accuracy of Validation data = {:2.1f}%\".format(score[1]*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/project_data/all_data\\1.JPG\n",
      "D:/project_data/all_data\\10.JPG\n",
      "D:/project_data/all_data\\100.JPG\n",
      "D:/project_data/all_data\\101.JPG\n",
      "D:/project_data/all_data\\102.JPG\n",
      "D:/project_data/all_data\\103.JPG\n",
      "D:/project_data/all_data\\104.JPG\n",
      "D:/project_data/all_data\\105.JPG\n",
      "D:/project_data/all_data\\106.JPG\n",
      "D:/project_data/all_data\\107.JPG\n",
      "D:/project_data/all_data\\108.JPG\n",
      "D:/project_data/all_data\\109.JPG\n",
      "D:/project_data/all_data\\11.JPG\n",
      "D:/project_data/all_data\\110.JPG\n",
      "D:/project_data/all_data\\112.JPG\n",
      "D:/project_data/all_data\\113.JPG\n",
      "D:/project_data/all_data\\114.JPG\n",
      "D:/project_data/all_data\\115.JPG\n",
      "D:/project_data/all_data\\116.JPG\n",
      "D:/project_data/all_data\\117.JPG\n",
      "D:/project_data/all_data\\118.JPG\n",
      "D:/project_data/all_data\\119.JPG\n",
      "D:/project_data/all_data\\12.JPG\n",
      "D:/project_data/all_data\\120.JPG\n",
      "D:/project_data/all_data\\121.JPG\n",
      "D:/project_data/all_data\\122.JPG\n",
      "D:/project_data/all_data\\123.JPG\n",
      "D:/project_data/all_data\\124.JPG\n",
      "D:/project_data/all_data\\125.JPG\n",
      "D:/project_data/all_data\\126.JPG\n",
      "D:/project_data/all_data\\127.JPG\n",
      "D:/project_data/all_data\\128.JPG\n",
      "D:/project_data/all_data\\129.JPG\n",
      "D:/project_data/all_data\\13.JPG\n",
      "D:/project_data/all_data\\130.JPG\n",
      "D:/project_data/all_data\\131.JPG\n",
      "D:/project_data/all_data\\132.JPG\n",
      "D:/project_data/all_data\\133.JPG\n",
      "D:/project_data/all_data\\134.JPG\n",
      "D:/project_data/all_data\\135.JPG\n",
      "D:/project_data/all_data\\136.JPG\n",
      "D:/project_data/all_data\\137.JPG\n",
      "D:/project_data/all_data\\138.JPG\n",
      "D:/project_data/all_data\\139.JPG\n",
      "D:/project_data/all_data\\14.JPG\n",
      "D:/project_data/all_data\\140.JPG\n",
      "D:/project_data/all_data\\141.JPG\n",
      "D:/project_data/all_data\\142.JPG\n",
      "D:/project_data/all_data\\143.JPG\n",
      "D:/project_data/all_data\\144.JPG\n",
      "D:/project_data/all_data\\145.JPG\n",
      "D:/project_data/all_data\\146.JPG\n",
      "D:/project_data/all_data\\147.JPG\n",
      "D:/project_data/all_data\\148.JPG\n",
      "D:/project_data/all_data\\149.JPG\n",
      "D:/project_data/all_data\\15.JPG\n",
      "D:/project_data/all_data\\150.JPG\n",
      "D:/project_data/all_data\\151.JPG\n",
      "D:/project_data/all_data\\152.JPG\n",
      "D:/project_data/all_data\\153.JPG\n",
      "D:/project_data/all_data\\154.JPG\n",
      "D:/project_data/all_data\\155.JPG\n",
      "D:/project_data/all_data\\156.JPG\n",
      "D:/project_data/all_data\\157.JPG\n",
      "D:/project_data/all_data\\158.JPG\n",
      "D:/project_data/all_data\\159.JPG\n",
      "D:/project_data/all_data\\16.JPG\n",
      "D:/project_data/all_data\\160.JPG\n",
      "D:/project_data/all_data\\161.JPG\n",
      "D:/project_data/all_data\\17.JPG\n",
      "D:/project_data/all_data\\18.JPG\n",
      "D:/project_data/all_data\\19.JPG\n",
      "D:/project_data/all_data\\2.JPG\n",
      "D:/project_data/all_data\\20.JPG\n",
      "D:/project_data/all_data\\21.JPG\n",
      "D:/project_data/all_data\\22.JPG\n",
      "D:/project_data/all_data\\23.JPG\n",
      "D:/project_data/all_data\\24.JPG\n",
      "D:/project_data/all_data\\25.JPG\n",
      "D:/project_data/all_data\\26.JPG\n",
      "D:/project_data/all_data\\27.JPG\n",
      "D:/project_data/all_data\\28.JPG\n",
      "D:/project_data/all_data\\29.JPG\n",
      "D:/project_data/all_data\\3.JPG\n",
      "D:/project_data/all_data\\30.JPG\n",
      "D:/project_data/all_data\\31.JPG\n",
      "D:/project_data/all_data\\32.JPG\n",
      "D:/project_data/all_data\\33.JPG\n",
      "D:/project_data/all_data\\34.JPG\n",
      "D:/project_data/all_data\\35.JPG\n",
      "D:/project_data/all_data\\36.JPG\n",
      "D:/project_data/all_data\\37.JPG\n",
      "D:/project_data/all_data\\38.JPG\n",
      "D:/project_data/all_data\\39.JPG\n",
      "D:/project_data/all_data\\4.JPG\n",
      "D:/project_data/all_data\\40.JPG\n",
      "D:/project_data/all_data\\41.JPG\n",
      "D:/project_data/all_data\\42.JPG\n",
      "D:/project_data/all_data\\43.JPG\n",
      "D:/project_data/all_data\\44.JPG\n",
      "D:/project_data/all_data\\45.JPG\n",
      "D:/project_data/all_data\\46.JPG\n",
      "D:/project_data/all_data\\47.JPG\n",
      "D:/project_data/all_data\\48.JPG\n",
      "D:/project_data/all_data\\49.JPG\n",
      "D:/project_data/all_data\\5.JPG\n",
      "D:/project_data/all_data\\50.JPG\n",
      "D:/project_data/all_data\\51.JPG\n",
      "D:/project_data/all_data\\52.JPG\n",
      "D:/project_data/all_data\\53.JPG\n",
      "D:/project_data/all_data\\54.JPG\n",
      "D:/project_data/all_data\\55.JPG\n",
      "D:/project_data/all_data\\56.JPG\n",
      "D:/project_data/all_data\\57.JPG\n",
      "D:/project_data/all_data\\58.JPG\n",
      "D:/project_data/all_data\\59.JPG\n",
      "D:/project_data/all_data\\6.JPG\n",
      "D:/project_data/all_data\\60.JPG\n",
      "D:/project_data/all_data\\61.JPG\n",
      "D:/project_data/all_data\\62.JPG\n",
      "D:/project_data/all_data\\63.JPG\n",
      "D:/project_data/all_data\\64.JPG\n",
      "D:/project_data/all_data\\65.JPG\n",
      "D:/project_data/all_data\\66.JPG\n",
      "D:/project_data/all_data\\67.JPG\n",
      "D:/project_data/all_data\\68.JPG\n",
      "D:/project_data/all_data\\69.JPG\n",
      "D:/project_data/all_data\\7.JPG\n",
      "D:/project_data/all_data\\70.JPG\n",
      "D:/project_data/all_data\\71.JPG\n",
      "D:/project_data/all_data\\72.JPG\n",
      "D:/project_data/all_data\\73.JPG\n",
      "D:/project_data/all_data\\74.JPG\n",
      "D:/project_data/all_data\\75.JPG\n",
      "D:/project_data/all_data\\76.JPG\n",
      "D:/project_data/all_data\\77.JPG\n",
      "D:/project_data/all_data\\78.JPG\n",
      "D:/project_data/all_data\\79.JPG\n",
      "D:/project_data/all_data\\8.JPG\n",
      "D:/project_data/all_data\\80.JPG\n",
      "D:/project_data/all_data\\81.JPG\n",
      "D:/project_data/all_data\\82.JPG\n",
      "D:/project_data/all_data\\83.JPG\n",
      "D:/project_data/all_data\\84.JPG\n",
      "D:/project_data/all_data\\85.JPG\n",
      "D:/project_data/all_data\\86.JPG\n",
      "D:/project_data/all_data\\87.JPG\n",
      "D:/project_data/all_data\\88.JPG\n",
      "D:/project_data/all_data\\89.JPG\n",
      "D:/project_data/all_data\\9.JPG\n",
      "D:/project_data/all_data\\90.JPG\n",
      "D:/project_data/all_data\\91.JPG\n",
      "D:/project_data/all_data\\92.JPG\n",
      "D:/project_data/all_data\\93.JPG\n",
      "D:/project_data/all_data\\94.JPG\n",
      "D:/project_data/all_data\\95.JPG\n",
      "D:/project_data/all_data\\96.JPG\n",
      "D:/project_data/all_data\\97.JPG\n",
      "D:/project_data/all_data\\98.JPG\n",
      "D:/project_data/all_data\\99.JPG\n",
      "160\n",
      "(120, 60, 60, 3)\n",
      "(40, 60, 60, 3)\n",
      "(120,)\n",
      "(40,)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 60, 60, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 60, 60, 3)    12          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 60, 60, 32)   896         batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 60, 60, 32)   896         batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 60, 60, 32)   128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 60, 60, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 60, 60, 32)   9248        batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 60, 60, 32)   9248        batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 60, 60, 32)   128         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 60, 60, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 30, 30, 32)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 30, 30, 32)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 30, 30, 32)   0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 30, 30, 32)   0           max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 28800)        0           dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 28800)        0           dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 128)          3686528     flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 128)          3686528     flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 128)          512         dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 128)          512         dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 128)          0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 128)          0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 1)            129         dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1)            129         dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_3 (Average)             (None, 1)            0           dense_10[0][0]                   \n",
      "                                                                 dense_12[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 7,395,150\n",
      "Trainable params: 7,394,376\n",
      "Non-trainable params: 774\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "12/12 [==============================] - 9s 773ms/step - loss: 0.7259 - acc: 0.5333 - val_loss: 1.8329 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 0.7144 - acc: 0.4500 - val_loss: 0.7099 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - 2s 198ms/step - loss: 0.7092 - acc: 0.5500 - val_loss: 0.7075 - val_acc: 0.5000\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - 2s 200ms/step - loss: 0.7081 - acc: 0.4167 - val_loss: 0.7054 - val_acc: 0.5000\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - 2s 203ms/step - loss: 0.7057 - acc: 0.4667 - val_loss: 0.7037 - val_acc: 0.5000\n",
      "Epoch 6/20\n",
      "12/12 [==============================] - 2s 199ms/step - loss: 0.7033 - acc: 0.4833 - val_loss: 0.7022 - val_acc: 0.5000\n",
      "Epoch 7/20\n",
      "12/12 [==============================] - 2s 196ms/step - loss: 0.7018 - acc: 0.5333 - val_loss: 0.7011 - val_acc: 0.5000\n",
      "Epoch 8/20\n",
      "12/12 [==============================] - 2s 196ms/step - loss: 0.7019 - acc: 0.4833 - val_loss: 0.7000 - val_acc: 0.5000\n",
      "Epoch 9/20\n",
      "12/12 [==============================] - 2s 196ms/step - loss: 0.7004 - acc: 0.5000 - val_loss: 0.6992 - val_acc: 0.5000\n",
      "Epoch 10/20\n",
      "12/12 [==============================] - 2s 200ms/step - loss: 0.7009 - acc: 0.4333 - val_loss: 0.6985 - val_acc: 0.5000\n",
      "Epoch 11/20\n",
      "12/12 [==============================] - 2s 200ms/step - loss: 0.6974 - acc: 0.5833 - val_loss: 0.6982 - val_acc: 0.5000\n",
      "Epoch 12/20\n",
      "12/12 [==============================] - 2s 200ms/step - loss: 0.7002 - acc: 0.4167 - val_loss: 0.6975 - val_acc: 0.5000\n",
      "Epoch 13/20\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 0.6977 - acc: 0.5000 - val_loss: 0.6971 - val_acc: 0.5000\n",
      "Epoch 14/20\n",
      "12/12 [==============================] - 2s 200ms/step - loss: 0.6987 - acc: 0.3667 - val_loss: 0.6967 - val_acc: 0.5000\n",
      "Epoch 15/20\n",
      "12/12 [==============================] - 2s 200ms/step - loss: 0.6971 - acc: 0.4667 - val_loss: 0.6963 - val_acc: 0.5000\n",
      "Epoch 16/20\n",
      "12/12 [==============================] - 2s 201ms/step - loss: 0.6978 - acc: 0.4000 - val_loss: 0.6960 - val_acc: 0.5000\n",
      "Epoch 17/20\n",
      "12/12 [==============================] - 2s 200ms/step - loss: 0.6966 - acc: 0.4833 - val_loss: 0.6957 - val_acc: 0.5000\n",
      "Epoch 18/20\n",
      "12/12 [==============================] - 2s 203ms/step - loss: 0.6979 - acc: 0.3167 - val_loss: 0.6954 - val_acc: 0.5000\n",
      "Epoch 19/20\n",
      "12/12 [==============================] - 2s 201ms/step - loss: 0.6962 - acc: 0.5000 - val_loss: 0.6952 - val_acc: 0.5000\n",
      "Epoch 20/20\n",
      "12/12 [==============================] - 2s 200ms/step - loss: 0.6977 - acc: 0.4333 - val_loss: 0.6949 - val_acc: 0.5000\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "Loss:  0.6948863506317139 \t[Info] Accuracy of Validation data = 50.0%\n"
     ]
    }
   ],
   "source": [
    "#first step import libary \n",
    "from keras import applications\n",
    "import cv2\n",
    "import glob\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D,  ZeroPadding2D, Input, average, Convolution2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.utils import np_utils # utilities for one-hot encoding of ground truth values\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2 # L2-regularisation\n",
    "\n",
    "\n",
    "#define all parameter for CNN\n",
    "\n",
    "batch_size = 5 # in each iteration, we consider 6 training examples at once\n",
    "epochs = 20 # we iterate ten times over the entire training set\n",
    "kernel_size = 3 # we will use 3x3 kernels throughout(like filter size for CNN)\n",
    "pool_size = 2 # we will use 2x2 pooling throughout\n",
    "conv_depth = 32 # use 32 kernels in both convolutional layers(use 32 filters)\n",
    "drop_prob_1 = 0.25 # dropout after pooling with probability 0.5\n",
    "drop_prob_2= 0.5 # dropout in the FC layer with probability 0.7\n",
    "img_width, img_height, img_depth = 60,60,3 # images are 150x150 and RGB\n",
    "#img_width, img_height = 150,150 # images are 150x150\n",
    "hidden_size = 128 #128 # there will be 128 neurons in both hidden layers(in Flatten layer)\n",
    "L2_lambda = 0.0001 # # use 0.0001 as a L2-regularisation factor(weight decay)\n",
    "num_classes = 1 # # there are 1 classes (1 per digit)\n",
    "\n",
    "ens_models = 2\n",
    "\n",
    "train_samples = 120    # training data samples count \n",
    "validation_samples = 40 # validation data samples count\n",
    "\n",
    "top_model_weights_path = 'bottleneck_fc_model_2.h5'\n",
    "\n",
    "#files = \"D:/project_data/all_data\"  # train data directory\n",
    "#validation_data = \"D:/project_data/validation\" #validation data directory\n",
    "\n",
    "data = []\n",
    "files = glob.glob (\"D:/project_data/all_data/*.jpg\")\n",
    "for myFile in files:\n",
    "    print(myFile)\n",
    "    image = cv2.imread (myFile)\n",
    "    data.append (image)\n",
    "\n",
    "filelist = glob.glob(\"D:/project_data/all_data/*.jpg\")\n",
    "data = np.array([np.array(Image.open(fname)) for fname in filelist])\n",
    "\n",
    "#data_label = np_utils.to_categorical(data, num_classes)\n",
    "\n",
    "data = np.resize(data.shape[0],(data.shape[0],img_width, img_height, img_depth))\n",
    "\n",
    "print(data.shape[0])\n",
    "\n",
    "train_data = data[:120]\n",
    "validation_data = data[120:]\n",
    "#X_train = X_train[:54000]\n",
    "#Y_train = Y_train[:54000]\n",
    "print(train_data.shape)\n",
    "print(validation_data.shape)\n",
    "#inp = Input(shape=(img_width, img_height, img_depth)) # N.B. TensorFlow back-end expects channel dimension last\n",
    "#inp_norm = BatchNormalization()(inp) # Apply BN to the input (N.B. need to rename here)\n",
    "\n",
    "#train_label = np_utils.to_categorical(len(train_data), num_classes) # One-hot encode the labels\n",
    "#validation_label = np_utils.to.categorical(len(validation_data), num_classes) # One-hot encode the labels\n",
    "\n",
    "inp = Input(shape=(img_width, img_height, img_depth)) # N.B. TensorFlow back-end expects channel dimension last\n",
    "inp_norm = BatchNormalization()(inp) # Apply BN to the input (N.B. need to rename here)\n",
    "\n",
    "\n",
    "train_labels = np.array(\n",
    "    [0] * (train_samples // 2) + [1] * (train_samples // 2))\n",
    "print(train_labels.shape)\n",
    "\n",
    "    #validation_data = np.load(open('bottleneck_features_validation.npy'))\n",
    "validation_labels = np.array(\n",
    "    [0] * (validation_samples // 2) + [1] * (validation_samples // 2))\n",
    "\n",
    "print(validation_labels.shape)\n",
    "\n",
    "#validation_labels = np_utils.to_categorical(validation_labels, num_classes) # One-hot encode the labels\n",
    "\n",
    "outs = [] # the list of ensemble outputs\n",
    "for i in range(ens_models):\n",
    "    # Conv [32] -> Conv [32] -> Pool (with dropout on the pooling layer), applying BN in between\n",
    "    conv_1 = Convolution2D(conv_depth, (kernel_size, kernel_size), \n",
    "        padding='same', \n",
    "        kernel_initializer='he_uniform', \n",
    "        kernel_regularizer=l2(L2_lambda), \n",
    "        activation='relu')(inp_norm)\n",
    "    conv_1 = BatchNormalization()(conv_1)\n",
    "    conv_2 = Convolution2D(conv_depth, (kernel_size, kernel_size), \n",
    "        padding='same', \n",
    "        kernel_initializer='he_uniform', \n",
    "        kernel_regularizer=l2(L2_lambda), \n",
    "        activation='relu')(conv_1)\n",
    "    conv_2 = BatchNormalization()(conv_2)\n",
    "    pool_1 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_2)\n",
    "    drop_1 = Dropout(drop_prob_1 )(pool_1)\n",
    "    flat = Flatten()(drop_1)\n",
    "    hidden = Dense(hidden_size, \n",
    "        kernel_initializer='he_uniform', \n",
    "        kernel_regularizer=l2(L2_lambda), \n",
    "        activation='relu')(flat) # Hidden ReLU layer\n",
    "    hidden = BatchNormalization()(hidden)\n",
    "    drop = Dropout(drop_prob_2)(hidden)\n",
    "    outs.append(Dense(num_classes, \n",
    "        kernel_initializer='glorot_uniform', \n",
    "        kernel_regularizer=l2(L2_lambda), \n",
    "        activation='sigmoid')(drop)) # Output softmax layer\n",
    "\n",
    "out = average(outs) # average the predictions to obtain the final output\n",
    "\n",
    "model = Model(inputs=inp, outputs=out) # To define a model, just specify its input and output layers\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1)  # randomly shift images vertically (fraction of total height)\n",
    "datagen.fit(train_data)\n",
    "\n",
    "model.summary()\n",
    "# fit the model on the batches generated by datagen.flow() - most parameters similar to model.fit\n",
    "model.fit_generator(datagen.flow(train_data, train_labels,\n",
    "                        batch_size=batch_size),\n",
    "                        steps_per_epoch=train_data.shape[0]//10,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(validation_data, validation_labels))\n",
    "                        #verbose=1,\n",
    "                        #callbacks=[EarlyStopping(monitor='val_loss', patience=5)]) # adding early stopping\n",
    "\n",
    "\n",
    "score = model.evaluate(validation_data,validation_labels)\n",
    "\n",
    "model.save_weights(top_model_weights_path)\n",
    "\n",
    "print(\"Loss: \", score[0], \"\\t[Info] Accuracy of Validation data = {:2.1f}%\".format(score[1]*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000, 28, 28, 1)\n",
      "(60000, 10)\n",
      "Epoch 1/2\n",
      "100/100 [==============================] - 56s 559ms/step - loss: 0.8683 - acc: 0.7803 - val_loss: 0.2878 - val_acc: 0.9648\n",
      "Epoch 2/2\n",
      "100/100 [==============================] - 49s 494ms/step - loss: 0.4511 - acc: 0.9185 - val_loss: 0.2339 - val_acc: 0.9765\n",
      "10000/10000 [==============================] - 16s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.23748987517356873, 0.9751]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.datasets import mnist # subroutines for fetching the MNIST dataset\n",
    "from keras.models import Model # basic class for specifying and training a neural network\n",
    "from keras.layers import Input, Dense, Flatten, Convolution2D, MaxPooling2D, Dropout, average\n",
    "from keras.utils import np_utils # utilities for one-hot encoding of ground truth values\n",
    "from keras.regularizers import l2 # L2-regularisation\n",
    "from keras.layers.normalization import BatchNormalization # batch normalisation\n",
    "from keras.preprocessing.image import ImageDataGenerator # data augmentation\n",
    "from keras.callbacks import EarlyStopping # early stopping\n",
    "\n",
    "batch_size = 128 # in each iteration, we consider 128 training examples at once\n",
    "num_epochs = 2 # we iterate at most fifty times over the entire training set\n",
    "kernel_size = 3 # we will use 3x3 kernels throughout\n",
    "pool_size = 2 # we will use 2x2 pooling throughout\n",
    "conv_depth = 32 # use 32 kernels in both convolutional layers\n",
    "drop_prob_1 = 0.25 # dropout after pooling with probability 0.25\n",
    "drop_prob_2 = 0.5 # dropout in the FC layer with probability 0.5\n",
    "hidden_size = 128 # there will be 128 neurons in both hidden layers\n",
    "l2_lambda = 0.0001 # use 0.0001 as a L2-regularisation factor\n",
    "ens_models = 3 # we will train three separate models on the data\n",
    "\n",
    "num_train = 60000 # there are 60000 training examples in MNIST\n",
    "num_test = 10000 # there are 10000 test examples in MNIST\n",
    "\n",
    "height, width, depth = 28, 28, 1 # MNIST images are 28x28 and greyscale\n",
    "num_classes = 10 # there are 10 classes (1 per digit)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() # fetch MNIST data\n",
    "print(X_train.shape)\n",
    "X_train = X_train.reshape(X_train.shape[0], height, width, depth)\n",
    "print(X_train.shape)\n",
    "X_test = X_test.reshape(X_test.shape[0], height, width, depth)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, num_classes) # One-hot encode the labels\n",
    "Y_test = np_utils.to_categorical(y_test, num_classes) # One-hot encode the labels\n",
    "\n",
    "print(Y_train.shape)\n",
    "# Explicitly split the training and validation sets\n",
    "X_val = X_train[54000:]\n",
    "Y_val = Y_train[54000:]\n",
    "X_train = X_train[:54000]\n",
    "Y_train = Y_train[:54000]\n",
    "\n",
    "inp = Input(shape=(height, width, depth)) # N.B. TensorFlow back-end expects channel dimension last\n",
    "inp_norm = BatchNormalization()(inp) # Apply BN to the input (N.B. need to rename here)\n",
    "\n",
    "outs = [] # the list of ensemble outputs\n",
    "for i in range(ens_models):\n",
    "    # Conv [32] -> Conv [32] -> Pool (with dropout on the pooling layer), applying BN in between\n",
    "    conv_1 = Convolution2D(conv_depth, (kernel_size, kernel_size), \n",
    "        padding='same', \n",
    "        kernel_initializer='he_uniform', \n",
    "        kernel_regularizer=l2(l2_lambda), \n",
    "        activation='relu')(inp_norm)\n",
    "    conv_1 = BatchNormalization()(conv_1)\n",
    "    conv_2 = Convolution2D(conv_depth, (kernel_size, kernel_size), \n",
    "        padding='same', \n",
    "        kernel_initializer='he_uniform', \n",
    "        kernel_regularizer=l2(l2_lambda), \n",
    "        activation='relu')(conv_1)\n",
    "    conv_2 = BatchNormalization()(conv_2)\n",
    "    pool_1 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_2)\n",
    "    drop_1 = Dropout(drop_prob_1)(pool_1)\n",
    "    flat = Flatten()(drop_1)\n",
    "    hidden = Dense(hidden_size, \n",
    "        kernel_initializer='he_uniform', \n",
    "        kernel_regularizer=l2(l2_lambda), \n",
    "        activation='relu')(flat) # Hidden ReLU layer\n",
    "    hidden = BatchNormalization()(hidden)\n",
    "    drop = Dropout(drop_prob_2)(hidden)\n",
    "    outs.append(Dense(num_classes, \n",
    "        kernel_initializer='glorot_uniform', \n",
    "        kernel_regularizer=l2(l2_lambda), \n",
    "        activation='softmax')(drop)) # Output softmax layer\n",
    "\n",
    "out = average(outs) # average the predictions to obtain the final output\n",
    "\n",
    "model = Model(inputs=inp, outputs=out) # To define a model, just specify its input and output layers\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', # using the cross-entropy loss function\n",
    "              optimizer='adam', # using the Adam optimiser\n",
    "              metrics=['accuracy']) # reporting the accuracy\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1)  # randomly shift images vertically (fraction of total height)\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# fit the model on the batches generated by datagen.flow() - most parameters similar to model.fit\n",
    "model.fit_generator(datagen.flow(X_train, Y_train,\n",
    "                        batch_size=batch_size),\n",
    "                        steps_per_epoch=X_train.shape[0]//540,\n",
    "                        epochs=num_epochs,\n",
    "                        validation_data=(X_val, Y_val),\n",
    "                        verbose=1,\n",
    "                        callbacks=[EarlyStopping(monitor='val_loss', patience=5)]) # adding early stopping\n",
    "\n",
    "model.evaluate(X_test, Y_test, verbose=1) # Evaluate the trained model on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\William\\Miniconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Found 120 images belonging to 2 classes.\n",
      "Found 40 images belonging to 2 classes.\n",
      "(120, 50, 50, 3)\n",
      "(40, 50, 50, 3)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 50, 50, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 50, 50, 3)    12          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 50, 50, 32)   896         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 50, 50, 32)   896         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 50, 50, 32)   128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 50, 50, 32)   128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 50, 50, 32)   9248        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 50, 50, 32)   9248        batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 50, 50, 32)   128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 50, 50, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 25, 25, 32)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 25, 25, 32)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 25, 25, 32)   0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 25, 25, 32)   0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 20000)        0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 20000)        0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          2560128     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          2560128     flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 128)          512         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128)          512         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            129         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            129         dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_1 (Average)             (None, 1)            0           dense_2[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,142,350\n",
      "Trainable params: 5,139,650\n",
      "Non-trainable params: 2,700\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "12/12 [==============================] - 8s 684ms/step - loss: 0.7500 - acc: 0.4583 - val_loss: 8.0932 - val_acc: 0.5000\n",
      "Epoch 2/50\n",
      "12/12 [==============================] - 2s 160ms/step - loss: 0.7194 - acc: 0.5000 - val_loss: 0.7148 - val_acc: 0.5000\n",
      "Epoch 3/50\n",
      "12/12 [==============================] - 2s 160ms/step - loss: 0.7141 - acc: 0.4917 - val_loss: 8.0790 - val_acc: 0.5000\n",
      "Epoch 4/50\n",
      "12/12 [==============================] - 2s 163ms/step - loss: 0.7132 - acc: 0.4417 - val_loss: 0.7111 - val_acc: 0.5000\n",
      "Epoch 5/50\n",
      "12/12 [==============================] - 2s 161ms/step - loss: 0.7109 - acc: 0.4750 - val_loss: 0.7097 - val_acc: 0.5000\n",
      "Epoch 6/50\n",
      "12/12 [==============================] - 2s 162ms/step - loss: 0.7096 - acc: 0.4583 - val_loss: 8.0747 - val_acc: 0.5000\n",
      "Epoch 7/50\n",
      "12/12 [==============================] - 2s 161ms/step - loss: 0.7088 - acc: 0.4667 - val_loss: 0.7081 - val_acc: 0.5000\n",
      "Epoch 8/50\n",
      "12/12 [==============================] - 2s 164ms/step - loss: 0.7079 - acc: 0.5167 - val_loss: 8.0735 - val_acc: 0.5000\n",
      "Epoch 9/50\n",
      "12/12 [==============================] - 2s 161ms/step - loss: 0.7076 - acc: 0.5000 - val_loss: 0.7072 - val_acc: 0.5000\n",
      "Epoch 10/50\n",
      "12/12 [==============================] - 2s 160ms/step - loss: 0.7075 - acc: 0.4167 - val_loss: 1.4504 - val_acc: 0.5000\n",
      "Epoch 11/50\n",
      "12/12 [==============================] - 2s 163ms/step - loss: 0.7073 - acc: 0.4417 - val_loss: 4.5271 - val_acc: 0.5000\n",
      "Epoch 12/50\n",
      "12/12 [==============================] - 2s 162ms/step - loss: 0.7073 - acc: 0.4000 - val_loss: 0.7308 - val_acc: 0.5000\n",
      "Epoch 13/50\n",
      "12/12 [==============================] - 2s 160ms/step - loss: 0.7068 - acc: 0.4917 - val_loss: 0.7228 - val_acc: 0.5000\n",
      "Epoch 14/50\n",
      "12/12 [==============================] - 2s 159ms/step - loss: 0.7074 - acc: 0.4583 - val_loss: 0.7144 - val_acc: 0.5000\n",
      "Epoch 15/50\n",
      "12/12 [==============================] - 2s 164ms/step - loss: 0.7070 - acc: 0.3667 - val_loss: 0.7122 - val_acc: 0.5000\n",
      "Epoch 16/50\n",
      "12/12 [==============================] - 2s 167ms/step - loss: 0.7064 - acc: 0.5250 - val_loss: 0.7064 - val_acc: 0.5000\n",
      "Epoch 17/50\n",
      "12/12 [==============================] - 2s 171ms/step - loss: 0.7063 - acc: 0.4500 - val_loss: 0.7064 - val_acc: 0.5000\n",
      "Epoch 18/50\n",
      "12/12 [==============================] - 2s 162ms/step - loss: 0.7064 - acc: 0.5500 - val_loss: 0.7063 - val_acc: 0.5000\n",
      "Epoch 19/50\n",
      "12/12 [==============================] - 2s 169ms/step - loss: 0.7068 - acc: 0.4083 - val_loss: 0.7063 - val_acc: 0.5000\n",
      "Epoch 20/50\n",
      "12/12 [==============================] - 2s 165ms/step - loss: 0.7060 - acc: 0.4833 - val_loss: 0.7063 - val_acc: 0.5000\n",
      "Epoch 21/50\n",
      "12/12 [==============================] - 2s 160ms/step - loss: 0.7062 - acc: 0.5167 - val_loss: 0.7063 - val_acc: 0.5000\n",
      "Epoch 22/50\n",
      "12/12 [==============================] - 2s 161ms/step - loss: 0.7067 - acc: 0.5167 - val_loss: 0.7063 - val_acc: 0.5000\n",
      "Epoch 23/50\n",
      "12/12 [==============================] - 2s 161ms/step - loss: 0.7072 - acc: 0.4833 - val_loss: 0.7063 - val_acc: 0.5000\n",
      "Epoch 24/50\n",
      "12/12 [==============================] - 2s 164ms/step - loss: 0.7074 - acc: 0.4583 - val_loss: 0.7063 - val_acc: 0.5000\n",
      "Epoch 25/50\n",
      "12/12 [==============================] - 2s 168ms/step - loss: 0.7075 - acc: 0.3917 - val_loss: 0.7063 - val_acc: 0.5000\n",
      "Epoch 26/50\n",
      "12/12 [==============================] - 2s 166ms/step - loss: 0.7065 - acc: 0.4917 - val_loss: 0.7063 - val_acc: 0.5000\n",
      "Epoch 27/50\n",
      "12/12 [==============================] - 2s 166ms/step - loss: 0.7074 - acc: 0.4167 - val_loss: 0.7063 - val_acc: 0.5000\n",
      "Epoch 28/50\n",
      "12/12 [==============================] - 2s 165ms/step - loss: 0.7063 - acc: 0.4583 - val_loss: 0.7063 - val_acc: 0.5000\n",
      "Epoch 29/50\n",
      "12/12 [==============================] - 2s 167ms/step - loss: 0.7073 - acc: 0.3250 - val_loss: 0.7062 - val_acc: 0.5000\n",
      "Epoch 30/50\n",
      "12/12 [==============================] - 2s 167ms/step - loss: 0.7059 - acc: 0.5333 - val_loss: 0.7062 - val_acc: 0.5000\n",
      "Epoch 31/50\n",
      "12/12 [==============================] - 2s 165ms/step - loss: 0.7072 - acc: 0.4500 - val_loss: 0.7062 - val_acc: 0.5000\n",
      "Epoch 32/50\n",
      "12/12 [==============================] - 2s 166ms/step - loss: 0.7065 - acc: 0.5000 - val_loss: 0.7062 - val_acc: 0.5000\n",
      "Epoch 33/50\n",
      "12/12 [==============================] - 2s 165ms/step - loss: 0.7059 - acc: 0.5750 - val_loss: 0.7062 - val_acc: 0.5000\n",
      "Epoch 34/50\n",
      "12/12 [==============================] - 2s 163ms/step - loss: 0.7072 - acc: 0.4500 - val_loss: 0.7063 - val_acc: 0.5000\n",
      "Epoch 35/50\n",
      "12/12 [==============================] - 2s 163ms/step - loss: 0.7062 - acc: 0.4917 - val_loss: 0.7062 - val_acc: 0.5000\n",
      "Epoch 36/50\n",
      "12/12 [==============================] - 2s 162ms/step - loss: 0.7066 - acc: 0.5417 - val_loss: 0.7062 - val_acc: 0.5000\n",
      "Epoch 37/50\n",
      "12/12 [==============================] - 2s 165ms/step - loss: 0.7062 - acc: 0.4667 - val_loss: 0.7062 - val_acc: 0.5000\n",
      "Epoch 38/50\n",
      "12/12 [==============================] - 2s 164ms/step - loss: 0.7068 - acc: 0.4583 - val_loss: 0.7062 - val_acc: 0.5000\n",
      "Epoch 39/50\n",
      "12/12 [==============================] - 2s 164ms/step - loss: 0.7063 - acc: 0.4750 - val_loss: 0.7062 - val_acc: 0.5000\n",
      "Epoch 40/50\n",
      "12/12 [==============================] - 2s 162ms/step - loss: 0.7070 - acc: 0.4000 - val_loss: 0.7062 - val_acc: 0.5000\n",
      "Epoch 41/50\n",
      "12/12 [==============================] - 2s 163ms/step - loss: 0.7069 - acc: 0.4250 - val_loss: 0.7062 - val_acc: 0.5000\n",
      "Epoch 42/50\n",
      "12/12 [==============================] - 2s 160ms/step - loss: 0.7066 - acc: 0.5750 - val_loss: 0.7062 - val_acc: 0.5000\n",
      "Epoch 43/50\n",
      "12/12 [==============================] - 2s 164ms/step - loss: 0.7060 - acc: 0.5083 - val_loss: 0.7062 - val_acc: 0.5000\n",
      "Epoch 44/50\n",
      "12/12 [==============================] - 2s 162ms/step - loss: 0.7068 - acc: 0.5417 - val_loss: 0.7062 - val_acc: 0.5000\n",
      "Epoch 45/50\n",
      "12/12 [==============================] - 2s 170ms/step - loss: 0.7063 - acc: 0.5167 - val_loss: 0.7062 - val_acc: 0.5000\n",
      "Epoch 46/50\n",
      "12/12 [==============================] - 2s 162ms/step - loss: 0.7066 - acc: 0.5167 - val_loss: 0.7062 - val_acc: 0.5000\n",
      "Epoch 47/50\n",
      "12/12 [==============================] - 2s 164ms/step - loss: 0.7065 - acc: 0.4583 - val_loss: 0.7062 - val_acc: 0.5000\n",
      "Epoch 48/50\n",
      "12/12 [==============================] - 2s 166ms/step - loss: 0.7056 - acc: 0.5500 - val_loss: 0.7062 - val_acc: 0.5000\n",
      "Epoch 49/50\n",
      "12/12 [==============================] - 2s 166ms/step - loss: 0.7059 - acc: 0.4833 - val_loss: 0.7062 - val_acc: 0.5000\n",
      "Epoch 50/50\n",
      "12/12 [==============================] - 2s 164ms/step - loss: 0.7067 - acc: 0.5167 - val_loss: 0.7062 - val_acc: 0.5000\n",
      "40/40 [==============================] - 0s 5ms/step\n",
      "Loss:  0.7062178611755371 \t[Info] Accuracy of Validation data = 50.0%\n"
     ]
    }
   ],
   "source": [
    "#first step import libary \n",
    "from keras import applications\n",
    "import cv2\n",
    "import glob\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D,  ZeroPadding2D, Input, average, Convolution2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.utils import np_utils # utilities for one-hot encoding of ground truth values\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2 # L2-regularisation\n",
    "\n",
    "\n",
    "#define all parameter for CNN\n",
    "\n",
    "batch_size = 10 # in each iteration, we consider 6 training examples at once\n",
    "epochs = 50 # we iterate ten times over the entire training set\n",
    "kernel_size = 3 # we will use 3x3 kernels throughout(like filter size for CNN)\n",
    "pool_size = 2 # we will use 2x2 pooling throughout\n",
    "conv_depth = 32 # use 32 kernels in both convolutional layers(use 32 filters)\n",
    "drop_prob_1 = 0.25 # dropout after pooling with probability 0.5\n",
    "drop_prob_2= 0.5 # dropout in the FC layer with probability 0.7\n",
    "img_width, img_height, img_depth = 50,50,3 # images are 150x150 and RGB\n",
    "#img_width, img_height = 150,150 # images are 150x150\n",
    "hidden_size = 128 #128 # there will be 128 neurons in both hidden layers(in Flatten layer)\n",
    "L2_lambda = 0.0001 # # use 0.0001 as a L2-regularisation factor(weight decay)\n",
    "num_classes = 1 # # there are 1 classes (1 per digit)\n",
    "\n",
    "ens_models = 2\n",
    "\n",
    "train_data = \"D:/project_data/train\"  # train data directory\n",
    "validation_data = \"D:/project_data/validation\" #validation data directory\n",
    "\n",
    "top_model_weights_path = 'bottleneck_fc_model_2.h5'\n",
    "\n",
    "train_samples = 120    # training data samples count \n",
    "validation_samples = 40 # validation data samples count\n",
    "\n",
    "#inp = Input(shape=(img_width, img_height, img_depth)) # N.B. TensorFlow back-end expects channel dimension last\n",
    "#inp_norm = BatchNormalization()(inp) # Apply BN to the input (N.B. need to rename here)\n",
    "\n",
    "\n",
    "inp = Input(shape=(img_width, img_height, img_depth)) # N.B. TensorFlow back-end expects channel dimension last\n",
    "inp_norm = BatchNormalization()(inp) # Apply BN to the input (N.B. need to rename here)\n",
    "\n",
    "#datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    # build the VGG16 network\n",
    "#model = applications.VGG16(include_top=False, weights='imagenet',input_shape=(60,60,3))\n",
    "model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "    # build the ResNet50 network\n",
    "    #model = applications.ResNet50(include_top=False, weights='imagenet')\n",
    "print('Model loaded.')\n",
    "\n",
    "generator = datagen.flow_from_directory(\n",
    "    train_data,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    #class_mode=None,\n",
    "    class_mode='binary',\n",
    "    shuffle=False)\n",
    "        #shuffle=True)\n",
    "bottleneck_features_train_2 = model.predict_generator(\n",
    "        generator, train_samples // batch_size)\n",
    "    #np.save(open('bottleneck_features_train.npy', 'w'),\n",
    "            #bottleneck_features_train)\n",
    "np.save('bottleneck_features_train_2.npy', bottleneck_features_train_2)\n",
    "generator = datagen.flow_from_directory(\n",
    "    validation_data,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "     #class_mode=None,\n",
    "    class_mode='binary',\n",
    "    shuffle=False)\n",
    "bottleneck_features_validation_2 = model.predict_generator(\n",
    "        generator, validation_samples // batch_size)\n",
    "    #np.save(open('bottleneck_features_validation.npy', 'w'),\n",
    "            #bottleneck_features_validation)\n",
    "np.save('bottleneck_features_validation_2.npy', bottleneck_features_validation_2)\n",
    "\n",
    "    #train_data = np.load(open('bottleneck_features_train.npy'))\n",
    "train_data = np.load('bottleneck_features_train_2.npy')\n",
    "#train_data = np.array(train_data)\n",
    "train_data = np.resize(train_data.shape[0],(train_data.shape[0],img_width, img_height, img_depth)) #resize dimension to (120,60,60,3)\n",
    "print(train_data.shape)\n",
    "train_labels = np.array(\n",
    "    [0] * (train_samples // 2) + [1] * (train_samples // 2))\n",
    "\n",
    "    #validation_data = np.load(open('bottleneck_features_validation.npy'))\n",
    "validation_data = np.load('bottleneck_features_validation_2.npy')\n",
    "#validation_data = np.array(validation_data)\n",
    "validation_data = np.resize(validation_data.shape[0],(validation_data.shape[0],img_width, img_height, img_depth))\n",
    "validation_labels = np.array(\n",
    "    [0] * (validation_samples // 2) + [1] * (validation_samples // 2))\n",
    "print(validation_data.shape)\n",
    "\n",
    "\n",
    "#validation_labels = np_utils.to_categorical(validation_labels, num_classes) # One-hot encode the labels\n",
    "\n",
    "outs = [] # the list of ensemble outputs\n",
    "for i in range(ens_models):\n",
    "    # Conv [32] -> Conv [32] -> Pool (with dropout on the pooling layer), applying BN in between\n",
    "    conv_1 = Convolution2D(conv_depth, (kernel_size, kernel_size), \n",
    "        padding='same', \n",
    "        kernel_initializer='he_uniform', \n",
    "        kernel_regularizer=l2(L2_lambda), \n",
    "        activation='relu')(inp_norm)\n",
    "    conv_1 = BatchNormalization()(conv_1)\n",
    "    conv_2 = Convolution2D(conv_depth, (kernel_size, kernel_size), \n",
    "        padding='same', \n",
    "        kernel_initializer='he_uniform', \n",
    "        kernel_regularizer=l2(L2_lambda), \n",
    "        activation='relu')(conv_1)\n",
    "    conv_2 = BatchNormalization()(conv_2)\n",
    "    pool_1 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_2)\n",
    "    drop_1 = Dropout(drop_prob_1 )(pool_1)\n",
    "    flat = Flatten()(drop_1)\n",
    "    hidden = Dense(hidden_size, \n",
    "        kernel_initializer='he_uniform', \n",
    "        kernel_regularizer=l2(L2_lambda), \n",
    "        activation='relu')(flat) # Hidden ReLU layer\n",
    "    hidden = BatchNormalization()(hidden)\n",
    "    drop = Dropout(drop_prob_2)(hidden)\n",
    "    outs.append(Dense(num_classes, \n",
    "        kernel_initializer='glorot_uniform', \n",
    "        kernel_regularizer=l2(L2_lambda), \n",
    "        activation='sigmoid')(drop)) # Output softmax layer\n",
    "\n",
    "out = average(outs) # average the predictions to obtain the final output\n",
    "\n",
    "model = Model(inputs=inp, outputs=out) # To define a model, just specify its input and output layers\n",
    "\n",
    "for layer in model.layers[:6]:\n",
    "    layer.trainable = False\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              #optimizer='rmsprop'\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1)  # randomly shift images vertically (fraction of total height)\n",
    "datagen.fit(train_data)\n",
    "\n",
    "model.summary()\n",
    "# fit the model on the batches generated by datagen.flow() - most parameters similar to model.fit\n",
    "model.fit_generator(datagen.flow(train_data, train_labels,\n",
    "                        batch_size=batch_size),\n",
    "                        steps_per_epoch=train_data.shape[0]//10,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(validation_data, validation_labels))\n",
    "                        #verbose=1,\n",
    "                        #callbacks=[EarlyStopping(monitor='val_loss', patience=5)]) # adding early stopping\n",
    "\n",
    "\n",
    "score = model.evaluate(validation_data,validation_labels)\n",
    "\n",
    "model.save_weights(top_model_weights_path)\n",
    "\n",
    "print(\"Loss: \", score[0], \"\\t[Info] Accuracy of Validation data = {:2.1f}%\".format(score[1]*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1800 images belonging to 2 classes.\n",
      "Found 480 images belonging to 2 classes.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 78, 78, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 78, 78, 3)    12          input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 78, 78, 32)   896         batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 78, 78, 32)   896         batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 78, 78, 32)   128         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 78, 78, 32)   128         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 78, 78, 32)   9248        batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 78, 78, 32)   9248        batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 78, 78, 32)   128         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 78, 78, 32)   128         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling2D) (None, 39, 39, 32)   0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling2D) (None, 39, 39, 32)   0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 39, 39, 32)   0           max_pooling2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 39, 39, 32)   0           max_pooling2d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 48672)        0           dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 48672)        0           dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 128)          6230144     flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 128)          6230144     flatten_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 128)          512         dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 128)          512         dense_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 128)          0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 128)          0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 1)            129         dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 1)            129         dropout_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_5 (Average)             (None, 1)            0           dense_30[0][0]                   \n",
      "                                                                 dense_32[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 12,482,382\n",
      "Trainable params: 12,481,608\n",
      "Non-trainable params: 774\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "75/75 [==============================] - 51s 685ms/step - loss: 0.6662 - acc: 0.7311 - val_loss: 1.4320 - val_acc: 0.4688\n",
      "Epoch 2/20\n",
      "75/75 [==============================] - 44s 591ms/step - loss: 0.5347 - acc: 0.8300 - val_loss: 1.7568 - val_acc: 0.5208\n",
      "Epoch 3/20\n",
      "75/75 [==============================] - 44s 591ms/step - loss: 0.4950 - acc: 0.8733 - val_loss: 1.9434 - val_acc: 0.5375\n",
      "Epoch 4/20\n",
      "75/75 [==============================] - 44s 591ms/step - loss: 0.5015 - acc: 0.8800 - val_loss: 1.2852 - val_acc: 0.5896\n",
      "Epoch 5/20\n",
      "75/75 [==============================] - 44s 592ms/step - loss: 0.4480 - acc: 0.9067 - val_loss: 1.4967 - val_acc: 0.6583\n",
      "Epoch 6/20\n",
      "75/75 [==============================] - 44s 592ms/step - loss: 0.4478 - acc: 0.9167 - val_loss: 2.0778 - val_acc: 0.5396\n",
      "Epoch 7/20\n",
      "75/75 [==============================] - 44s 593ms/step - loss: 0.4418 - acc: 0.9333 - val_loss: 1.6592 - val_acc: 0.5604\n",
      "Epoch 8/20\n",
      "75/75 [==============================] - 45s 596ms/step - loss: 0.4457 - acc: 0.9222 - val_loss: 1.7134 - val_acc: 0.6167\n",
      "Epoch 9/20\n",
      "75/75 [==============================] - 44s 592ms/step - loss: 0.4113 - acc: 0.9394 - val_loss: 1.4791 - val_acc: 0.6458\n",
      "Epoch 10/20\n",
      "75/75 [==============================] - 44s 591ms/step - loss: 0.4354 - acc: 0.9289 - val_loss: 1.4603 - val_acc: 0.6271\n",
      "Epoch 11/20\n",
      "75/75 [==============================] - 44s 591ms/step - loss: 0.4135 - acc: 0.9400 - val_loss: 1.9971 - val_acc: 0.5271\n",
      "Epoch 12/20\n",
      "75/75 [==============================] - 44s 592ms/step - loss: 0.3968 - acc: 0.9467 - val_loss: 1.8551 - val_acc: 0.5792\n",
      "Epoch 13/20\n",
      "75/75 [==============================] - 44s 591ms/step - loss: 0.3805 - acc: 0.9539 - val_loss: 1.5611 - val_acc: 0.5958\n",
      "Epoch 14/20\n",
      "75/75 [==============================] - 44s 591ms/step - loss: 0.4106 - acc: 0.9389 - val_loss: 1.8675 - val_acc: 0.5917\n",
      "Epoch 15/20\n",
      "75/75 [==============================] - 44s 592ms/step - loss: 0.4126 - acc: 0.9461 - val_loss: 1.8338 - val_acc: 0.5604\n",
      "Epoch 16/20\n",
      "75/75 [==============================] - 45s 595ms/step - loss: 0.3955 - acc: 0.9572 - val_loss: 2.3904 - val_acc: 0.5708\n",
      "Epoch 17/20\n",
      "75/75 [==============================] - 44s 592ms/step - loss: 0.3750 - acc: 0.9500 - val_loss: 1.9253 - val_acc: 0.5771\n",
      "Epoch 18/20\n",
      "75/75 [==============================] - 44s 592ms/step - loss: 0.3574 - acc: 0.9650 - val_loss: 2.9059 - val_acc: 0.5521\n",
      "Epoch 19/20\n",
      "75/75 [==============================] - 44s 592ms/step - loss: 0.4004 - acc: 0.9494 - val_loss: 1.7309 - val_acc: 0.5854\n",
      "Epoch 20/20\n",
      "75/75 [==============================] - 45s 594ms/step - loss: 0.3714 - acc: 0.9556 - val_loss: 1.6372 - val_acc: 0.6063\n",
      "Loss:  1.63720945417881 \t[Info] Accuracy of Validation data = 60.6%\n"
     ]
    }
   ],
   "source": [
    "#first step import libary \n",
    "from keras import applications\n",
    "import cv2\n",
    "import glob\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D,  ZeroPadding2D, Input, average, Convolution2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.utils import np_utils # utilities for one-hot encoding of ground truth values\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2 # L2-regularisation\n",
    "\n",
    "\n",
    "#define all parameter for CNN\n",
    "\n",
    "batch_size = 24 # in each iteration, we consider 6 training examples at once\n",
    "epochs = 20 # we iterate ten times over the entire training set\n",
    "kernel_size = 3 # we will use 3x3 kernels throughout(like filter size for CNN)\n",
    "pool_size = 2 # we will use 2x2 pooling throughout\n",
    "conv_depth = 32 # use 32 kernels in both convolutional layers(use 32 filters)\n",
    "drop_prob_1 = 0.5 # dropout after pooling with probability 0.5\n",
    "drop_prob_2= 0.7 # dropout in the FC layer with probability 0.7\n",
    "img_width, img_height, img_depth = 78,78,3 # images are 150x150 and RGB\n",
    "#img_width, img_height = 150,150 # images are 150x150\n",
    "hidden_size = 128 #128 # there will be 128 neurons in both hidden layers(in Flatten layer)\n",
    "L2_lambda = 0.0001 # # use 0.0001 as a L2-regularisation factor(weight decay)\n",
    "num_classes = 1 # # there are 1 classes (1 per digit)\n",
    "\n",
    "ens_models = 2\n",
    "\n",
    "#train_data = \"D:/project_data/train\"  # train data directory\n",
    "#validation_data = \"D:/project_data/validation\" #validation data directory\n",
    "\n",
    "train_data = \"D:/project_data/treat/train\"\n",
    "validation_data = \"D:/project_data/treat/validation\"\n",
    "\n",
    "top_model_weights_path = 'bottleneck_fc_model_2.h5'\n",
    "\n",
    "train_samples = 1800    # training data samples count \n",
    "validation_samples = 480 # validation data samples count\n",
    "\n",
    "#inp = Input(shape=(img_width, img_height, img_depth)) # N.B. TensorFlow back-end expects channel dimension last\n",
    "#inp_norm = BatchNormalization()(inp) # Apply BN to the input (N.B. need to rename here)\n",
    "\n",
    "\n",
    "inp = Input(shape=(img_width, img_height, img_depth)) # N.B. TensorFlow back-end expects channel dimension last\n",
    "inp_norm = BatchNormalization()(inp) # Apply BN to the input (N.B. need to rename here)\n",
    "\n",
    "#datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "\n",
    "\n",
    "#validation_labels = np_utils.to_categorical(validation_labels, num_classes) # One-hot encode the labels\n",
    "\n",
    "outs = [] # the list of ensemble outputs\n",
    "for i in range(ens_models):\n",
    "    # Conv [32] -> Conv [32] -> Pool (with dropout on the pooling layer), applying BN in between\n",
    "    conv_1 = Convolution2D(conv_depth, (kernel_size, kernel_size), \n",
    "        padding='same', \n",
    "        kernel_initializer='he_uniform', \n",
    "        kernel_regularizer=l2(L2_lambda), \n",
    "        activation='relu')(inp_norm)\n",
    "    conv_1 = BatchNormalization()(conv_1)\n",
    "    conv_2 = Convolution2D(conv_depth, (kernel_size, kernel_size), \n",
    "        padding='same', \n",
    "        kernel_initializer='he_uniform', \n",
    "        kernel_regularizer=l2(L2_lambda), \n",
    "        activation='relu')(conv_1)\n",
    "    conv_2 = BatchNormalization()(conv_2)\n",
    "    pool_1 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_2)\n",
    "    drop_1 = Dropout(drop_prob_1 )(pool_1)\n",
    "    flat = Flatten()(drop_1)\n",
    "    hidden = Dense(hidden_size, \n",
    "        kernel_initializer='he_uniform', \n",
    "        kernel_regularizer=l2(L2_lambda), \n",
    "        activation='relu')(flat) # Hidden ReLU layer\n",
    "    hidden = BatchNormalization()(hidden)\n",
    "    drop = Dropout(drop_prob_2)(hidden)\n",
    "    outs.append(Dense(num_classes, \n",
    "        kernel_initializer='glorot_uniform', \n",
    "        kernel_regularizer=l2(L2_lambda), \n",
    "        activation='sigmoid')(drop)) # Output softmax layer\n",
    "    \n",
    "#model.save_weights(top_model_weights_path)\n",
    "\n",
    "out = average(outs) # average the predictions to obtain the final output\n",
    "\n",
    "model = Model(inputs=inp, outputs=out) # To define a model, just specify its input and output layers\n",
    "\n",
    "\n",
    "#for layer in model.layers[:6]:\n",
    "    #layer.trainable = False\n",
    "    \n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              #optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "#datagen = ImageDataGenerator(\n",
    "        #width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        #height_shift_range=0.1)  # randomly shift images vertically (fraction of total height)\n",
    "#datagen.fit(train_data)\n",
    "\n",
    "model.summary()\n",
    "# fit the model on the batches generated by datagen.flow() - most parameters similar to model.fit\n",
    "#model.fit_generator(datagen.flow(train_data, train_labels,\n",
    "                        #batch_size=batch_size),\n",
    "                        #steps_per_epoch=train_data.shape[0]//10,\n",
    "                        #epochs=epochs,\n",
    "                        #validation_data=(validation_data, validation_labels))\n",
    "                        #verbose=1,\n",
    "                        #callbacks=[EarlyStopping(monitor='val_loss', patience=5)]) # adding early stopping\n",
    "model.fit_generator(\n",
    "              train_generator,\n",
    "              steps_per_epoch=train_samples//batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=validation_generator,\n",
    "              validation_steps=validation_samples//batch_size)\n",
    "\n",
    "#score = model.evaluate(validation_data,validation_labels)\n",
    "score = model.evaluate_generator(validation_generator, validation_samples/batch_size)\n",
    "#scores = model.predict_generator(validation_generator, validation_samples/batch_size)\n",
    "\n",
    "#model.save_weights(top_model_weights_path)\n",
    "\n",
    "print(\"Loss: \", score[0], \"\\t[Info] Accuracy of Validation data = {:2.1f}%\".format(score[1]*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 120 images belonging to 2 classes.\n",
      "Found 40 images belonging to 2 classes.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to load a weight file containing 22 layers into a model with 15 layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1f4ff8eb115d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    107\u001b[0m         activation='sigmoid')(drop)) # Output softmax layer\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_model_weights_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# average the predictions to obtain the final output\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[0;32m   2643\u001b[0m                 f, self.layers, skip_mismatch=skip_mismatch)\n\u001b[0;32m   2644\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2645\u001b[1;33m             \u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2647\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'close'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[1;34m(f, layers)\u001b[0m\n\u001b[0;32m   3137\u001b[0m                          \u001b[1;34m'containing '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3138\u001b[0m                          \u001b[1;34m' layers into a model with '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3139\u001b[1;33m                          str(len(filtered_layers)) + ' layers.')\n\u001b[0m\u001b[0;32m   3140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3141\u001b[0m     \u001b[1;31m# We batch weight value assignments in a single backend call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to load a weight file containing 22 layers into a model with 15 layers."
     ]
    }
   ],
   "source": [
    "#first step import libary \n",
    "from keras import applications\n",
    "import cv2\n",
    "import glob\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D,  ZeroPadding2D, Input, average, Convolution2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.utils import np_utils # utilities for one-hot encoding of ground truth values\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2 # L2-regularisation\n",
    "\n",
    "\n",
    "#define all parameter for CNN\n",
    "\n",
    "batch_size = 8 # in each iteration, we consider 6 training examples at once\n",
    "epochs = 50 # we iterate ten times over the entire training set\n",
    "kernel_size = 3 # we will use 3x3 kernels throughout(like filter size for CNN)\n",
    "pool_size = 2 # we will use 2x2 pooling throughout\n",
    "conv_depth = 32 # use 32 kernels in both convolutional layers(use 32 filters)\n",
    "drop_prob_1 = 0.25 # dropout after pooling with probability 0.5\n",
    "drop_prob_2= 0.5 # dropout in the FC layer with probability 0.7\n",
    "img_width, img_height, img_depth = 78,78,3 # images are 150x150 and RGB\n",
    "#img_width, img_height = 150,150 # images are 150x150\n",
    "hidden_size = 128 #128 # there will be 128 neurons in both hidden layers(in Flatten layer)\n",
    "L2_lambda = 0.0001 # # use 0.0001 as a L2-regularisation factor(weight decay)\n",
    "num_classes = 1 # # there are 1 classes (1 per digit)\n",
    "\n",
    "ens_models = 2\n",
    "\n",
    "train_data = \"D:/project_data/train\"  # train data directory\n",
    "validation_data = \"D:/project_data/validation\" #validation data directory\n",
    "\n",
    "#top_model_weights_path = 'bottleneck_fc_model_2.h5'\n",
    "\n",
    "train_samples = 120    # training data samples count \n",
    "validation_samples = 40 # validation data samples count\n",
    "\n",
    "#inp = Input(shape=(img_width, img_height, img_depth)) # N.B. TensorFlow back-end expects channel dimension last\n",
    "#inp_norm = BatchNormalization()(inp) # Apply BN to the input (N.B. need to rename here)\n",
    "\n",
    "\n",
    "inp = Input(shape=(img_width, img_height, img_depth)) # N.B. TensorFlow back-end expects channel dimension last\n",
    "inp_norm = BatchNormalization()(inp) # Apply BN to the input (N.B. need to rename here)\n",
    "\n",
    "#datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "\n",
    "\n",
    "#validation_labels = np_utils.to_categorical(validation_labels, num_classes) # One-hot encode the labels\n",
    "\n",
    "outs = [] # the list of ensemble outputs\n",
    "for i in range(ens_models):\n",
    "    # Conv [32] -> Conv [32] -> Pool (with dropout on the pooling layer), applying BN in between\n",
    "    conv_1 = Convolution2D(conv_depth, (kernel_size, kernel_size), \n",
    "        padding='same', \n",
    "        kernel_initializer='he_uniform', \n",
    "        kernel_regularizer=l2(L2_lambda), \n",
    "        activation='relu')(inp_norm)\n",
    "    conv_1 = BatchNormalization()(conv_1)\n",
    "    conv_2 = Convolution2D(conv_depth, (kernel_size, kernel_size), \n",
    "        padding='same', \n",
    "        kernel_initializer='he_uniform', \n",
    "        kernel_regularizer=l2(L2_lambda), \n",
    "        activation='relu')(conv_1)\n",
    "    conv_2 = BatchNormalization()(conv_2)\n",
    "    pool_1 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_2)\n",
    "    drop_1 = Dropout(drop_prob_1 )(pool_1)\n",
    "    flat = Flatten()(drop_1)\n",
    "    hidden = Dense(hidden_size, \n",
    "        kernel_initializer='he_uniform', \n",
    "        kernel_regularizer=l2(L2_lambda), \n",
    "        activation='relu')(flat) # Hidden ReLU layer\n",
    "    hidden = BatchNormalization()(hidden)\n",
    "    drop = Dropout(drop_prob_2)(hidden)\n",
    "    outs.append(Dense(num_classes, \n",
    "        kernel_initializer='glorot_uniform', \n",
    "        kernel_regularizer=l2(L2_lambda), \n",
    "        activation='sigmoid')(drop)) # Output softmax layer\n",
    "    \n",
    "#model.load_weights(top_model_weights_path)\n",
    "\n",
    "out = average(outs) # average the predictions to obtain the final output\n",
    "\n",
    "model = Model(inputs=inp, outputs=out) # To define a model, just specify its input and output layers\n",
    "\n",
    "#model.load_weights(top_model_weights_path)\n",
    "\n",
    "#for layer in model.layers[:6]:\n",
    "    #layer.trainable = False\n",
    "    \n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              #optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "#datagen = ImageDataGenerator(\n",
    "        #width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        #height_shift_range=0.1)  # randomly shift images vertically (fraction of total height)\n",
    "#datagen.fit(train_data)\n",
    "\n",
    "model.summary()\n",
    "# fit the model on the batches generated by datagen.flow() - most parameters similar to model.fit\n",
    "#model.fit_generator(datagen.flow(train_data, train_labels,\n",
    "                        #batch_size=batch_size),\n",
    "                        #steps_per_epoch=train_data.shape[0]//10,\n",
    "                        #epochs=epochs,\n",
    "                        #validation_data=(validation_data, validation_labels))\n",
    "                        #verbose=1,\n",
    "                        #callbacks=[EarlyStopping(monitor='val_loss', patience=5)]) # adding early stopping\n",
    "model.fit_generator(\n",
    "              train_generator,\n",
    "              steps_per_epoch=train_samples//batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=validation_generator,\n",
    "              validation_steps=validation_samples//batch_size)\n",
    "\n",
    "#score = model.evaluate(validation_data,validation_labels)\n",
    "score = model.evaluate_generator(validation_generator, validation_samples/batch_size)\n",
    "#scores = model.predict_generator(validation_generator, validation_samples/batch_size)\n",
    "\n",
    "#model.save_weights(top_model_weights_path)\n",
    "\n",
    "print(\"Loss: \", score[0], \"\\t[Info] Accuracy of Validation data = {:2.1f}%\".format(score[1]*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NG\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "from tkinter import *\n",
    "import tkinter as tk\n",
    "from pytube import *\n",
    "import tkinter\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "#top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "top_model_weights_path = 'bottleneck_fc_model_1.h5'\n",
    "#train_data_dir = 'data/train'\n",
    "#validation_data_dir = 'data/validation'\n",
    "train_data_dir = \"D:/project_data/treat/train\"\n",
    "validation_data_dir = \"D:/project_data/treat/validation\"\n",
    "\n",
    "#train_data_dir = \"D:/project_data/train\"\n",
    "#validation_data_dir = \"D:/project_data/validation\"\n",
    "test_data_dir = \"D:/project_data/treat\"\n",
    "\n",
    "#nb_train_samples = 2000\n",
    "nb_train_samples = 1800\n",
    "#nb_train_samples = 120\n",
    "#nb_validation_samples = 800\n",
    "nb_validation_samples = 480\n",
    "#nb_validation_samples = 40\n",
    "epochs = 50\n",
    "batch_size = 24\n",
    "\n",
    "\n",
    "def save_bottlebeck_features():\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    # build the VGG16 network\n",
    "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "    # build the ResNet50 network\n",
    "    #model = applications.ResNet50(include_top=False, weights='imagenet')\n",
    "    print('Model loaded.')\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "        #shuffle=True)\n",
    "    bottleneck_features_train_1 = model.predict_generator(\n",
    "        generator, nb_train_samples // batch_size)\n",
    "    #np.save(open('bottleneck_features_train.npy', 'w'),\n",
    "            #bottleneck_features_train)\n",
    "    np.save('bottleneck_features_train_1.npy', bottleneck_features_train_1)\n",
    "    generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    bottleneck_features_validation_1 = model.predict_generator(\n",
    "        generator, nb_validation_samples // batch_size)\n",
    "    #np.save(open('bottleneck_features_validation.npy', 'w'),\n",
    "            #bottleneck_features_validation)\n",
    "    np.save('bottleneck_features_validation_1.npy', bottleneck_features_validation_1)\n",
    "\n",
    "def train_top_model():\n",
    "    #train_data = np.load(open('bottleneck_features_train.npy'))\n",
    "    train_data = np.load('bottleneck_features_train_1.npy')\n",
    "    train_labels = np.array(\n",
    "        [0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))\n",
    "\n",
    "    #validation_data = np.load(open('bottleneck_features_validation.npy'))\n",
    "    validation_data = np.load('bottleneck_features_validation_1.npy')\n",
    "    validation_labels = np.array(\n",
    "        [0] * (nb_validation_samples // 2) + [1] * (nb_validation_samples // 2))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.7))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    train_history= model.fit(train_data, train_labels,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            validation_data=(validation_data, validation_labels))\n",
    "    score = model.evaluate(validation_data,validation_labels)\n",
    "    #score = model.evaluate(validation_data,validation_labels, nb_validation_samples/batch_size)\n",
    "    #scores = model.predict(validation_data,validation_labels, nb_validation_samples/batch_size)\n",
    "\n",
    "    print(\"Loss: \", score[0], \"\\t[Info] Accuracy of Validation data = {:2.1f}%\".format(score[1]*100.0))\n",
    "\n",
    "    model.save_weights(top_model_weights_path)\n",
    "    \n",
    "\n",
    "#save_bottlebeck_features()\n",
    "#train_top_model()\n",
    "\n",
    "\n",
    "\n",
    "def predict_image_class(file):\n",
    "\n",
    "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "    x = load_img(file, target_size=(img_width,img_height))\n",
    "\n",
    "    x = img_to_array(x)\n",
    "\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "\n",
    "    array = model.predict(x)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Flatten(input_shape=array.shape[1:]))\n",
    "\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "\n",
    "    model.add(Dropout(0.5)) \n",
    "    #model.add(Dense(256, activation='relu'))\n",
    "\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.load_weights(top_model_weights_path)\n",
    "    #model.load_weights(weight_path)\n",
    "    #model.load_weights(Keras_weight_path)\n",
    "\n",
    "    class_predicted = model.predict_classes(array)\n",
    "\n",
    "    if class_predicted==1:\n",
    "\n",
    "        print(\"OK\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(\"NG\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "save_bottlebeck_features()\n",
    "train_top_model()\n",
    "\"\"\"\n",
    "top=tkinter.Tk()\n",
    "yt_variable = StringVar()\n",
    "def openFile(): \n",
    "    #messagebox.showwinfo(\"開啟舊檔\")\n",
    "    yt = yt_variable.get()\n",
    "    #predict_image_class(test_data_dir + \"/OK_0_1051.jpeg\" )\n",
    "    predict_image_class(yt)\n",
    "    #url = tk.StringVar(predict_image_class())\n",
    "    #predict_image_class(url )\n",
    "    #video = tk.IntVar()  #選項按鈕值\n",
    "    #url = tk.StringVar()  #影片網址\n",
    "    #url = tk.Entry()\n",
    "    #path = tk.StringVar()  #存檔資料夾\n",
    "    #filename = tk.StringVar()  #存檔名稱\n",
    "    \n",
    "#window = Tk()\n",
    "#menu = Menu(window)\n",
    "#window[\"menu\"]=menu\n",
    "#filemenu = Menu(menu)\n",
    "#menu.add_cascade(label=\"檔案\",menu=filemenu)\n",
    "#filemenu.add_command(label = \"開啟舊檔...\",command = openFile)\n",
    "#window.mainloop()\n",
    "#predict_image_class(test_data_dir + \"/OK_93.JPG\")\n",
    "\n",
    "yt_label=tkinter.Label(top,text='Image Link + Press Go',width=30)\n",
    "yt_label.pack()\n",
    "yt_button=tkinter.Button(top,text='Go',command= openFile,width=10)\n",
    "yt_button.pack()\n",
    "yt_entry=tkinter.Entry(top,textvariable=yt_variable,width=100)\n",
    "yt_entry.get()\n",
    "yt_entry.pack()\n",
    "\n",
    "top.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
